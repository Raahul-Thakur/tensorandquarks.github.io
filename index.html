<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Home | Tensors & Quarks</title>
    <link rel="stylesheet" href="/tensorandquarks.github.io/assets/style.css" />
    <script>
      // Dark mode script
      function toggleDarkMode() {
        document.body.classList.toggle("dark");
        localStorage.setItem("theme", document.body.classList.contains("dark") ? "dark" : "light");
      }

      window.onload = () => {
        if (localStorage.getItem("theme") === "dark") {
          document.body.classList.add("dark");
        }
      };
    </script>
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Home | Tensors &amp; Quarks</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Home" />
<meta name="author" content="Rahul Thakur" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Exploring the intersection of physics and machine learning." />
<meta property="og:description" content="Exploring the intersection of physics and machine learning." />
<link rel="canonical" href="https://raahul-thakur.github.io/tensorandquarks.github.io/" />
<meta property="og:url" content="https://raahul-thakur.github.io/tensorandquarks.github.io/" />
<meta property="og:site_name" content="Tensors &amp; Quarks" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Home" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","author":{"@type":"Person","name":"Rahul Thakur"},"description":"Exploring the intersection of physics and machine learning.","headline":"Home","name":"Tensors &amp; Quarks","url":"https://raahul-thakur.github.io/tensorandquarks.github.io/"}</script>
<!-- End Jekyll SEO tag -->

  </head>
  <body>
    <header class="site-header">
      <div class="container header-flex">
        <div class="branding">
          <img src="/tensorandquarks.github.io/assets/images/bio-photo.jpeg" alt="Rahul" class="profile-pic">
          <h1 class="site-title">Tensors & Quarks</h1>
        </div>
        <nav class="nav">
          <a href="/tensorandquarks.github.io/">Home</a>
          <a href="/tensorandquarks.github.io/about.html">About</a>
          <button onclick="toggleDarkMode()">üåì</button>
        </nav>
      </div>
    </header>
    <main class="container">
      <div class="hero">
  <h1>Welcome to Tensors &amp; Quarks</h1>
  <p>Exploring the cosmos of Physics &amp; the depths of Machine Learning.</p>
</div>

<h2>Latest Posts</h2>

<ul class="post-list">
  
    <li class="post-card">
      <h2><a href="/tensorandquarks.github.io/2025/02/13/tpa.html"></a></h2>

      <p class="post-meta">
        February 13, 2025
        
           <span class="inline-tag">ML</span>
        
      </p>

      <p><h1 id="from-heads-to-factors-a-deep-dive-into-tensor-product-attention-and-the-t6-transformer">From Heads to Factors: A Deep Dive into Tensor Product Attention and the T6 Transformer</h1>

<p><em>A Transformer layer must preserve every key‚Äìvalue pair for every head, layer, and past token‚Äîa memory bill that rises linearly with context length.</em>
<img src="https://upload.wikimedia.org/wikipedia/commons/7/74/Encoder_self-attention%2C_detailed_diagram.png" alt="Self-attention overview" /></p>

<hr />

<h2 id="1why-the-kv-cache-became-public-enemy-1">1‚ÄÇWhy the KV Cache Became Public Enemy #1</h2>

<p>Modern language models rarely choke on parameter counts; they choke on <strong>context</strong>.<br />
During autoregressive generation the model must keep <strong>two</strong> tensors‚Äîkeys and values‚Äîfor <strong>every</strong> past token so that future queries can look back.<br />
With <em>H</em> heads of size <em>d<sub>h</sub></em> and a sequence of <em>T</em> tokens, the cached footprint for a single layer is</p>

<p>[
\boxed{\text{Memory}<em>\text{MHA}=2\,H\,d</em>{h}\,T}\tag{1}
]</p>

<p>On an A100 GPU you can exhaust 40 GB of RAM long before you fill the compute budget, forcing practitioners to trim prompts, chop documents, or shard the KV cache across devices.<br />
Previous fixes (Multi-Query, Grouped-Query, Multi-head Latent Attention) either share keys between heads or summarise them into latents, clawing back memory at the price of modelling fidelity. <strong>Tensor Product Attention (TPA)</strong>‚Äîintroduced in <em>‚ÄúTensor Product Attention Is All You Need‚Äù</em> (2025)‚Äîoffers a radical alternative: <strong>compress</strong> each Q, K, V with a low-rank tensor factorisation so you keep almost all capacity while paying a fraction of the memory bill.</p>

<hr />

<h2 id="2a-one-equation-recap-of-multi-head-attention">2‚ÄÇA One-Equation Recap of Multi-Head Attention</h2>

<p>For a single head the scaled dot-product attention is</p>

<p>[
\mathrm{Attn}(Q,K,V)=\operatorname{softmax}!\Bigl(\tfrac{QK^{\top}}{\sqrt{d_{h}}}\Bigr)V.\tag{2}
]</p>

<p>Multi-Head Attention (MHA) simply repeats that calculation <em>H</em> times with separate projections.<br />
Expressive? Absolutely.<br />
But each head carries its own key‚Äìvalue history, so Equation (1) still looms large in memory-bound deployments.</p>

<hr />

<h2 id="3tensor-product-attention--compress-keys-keep-power">3‚ÄÇTensor Product Attention ‚Äî Compress Keys, Keep Power</h2>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/2/26/AM_diagrams_outer_product.svg" alt="Outer-product factorisation" />
<em>A rank-1 outer product; TPA stores a <strong>sum</strong> of such products rather than a full matrix.</em></p>

<p>The core idea is tantalisingly simple.<br />
Instead of caching a full <em>H √ó d<sub>h</sub></em> key matrix for every token, TPA writes it as a <strong>low-rank sum of outer products</strong></p>

<p>[
K=\sum_{r=1}^{R} a_{r}\;\otimes\;b_{r}^{\top},\tag{3}
]</p>

<p>where</p>

<ul>
  <li>(a_{r}\in\mathbb{R}^{H}) are <strong>head-factors</strong> that vary across heads but not token features,</li>
  <li>(b_{r}\in\mathbb{R}^{d_{h}}) are <strong>token-factors</strong> that vary across features but not heads,</li>
  <li>(R) ‚â™ (H) is the chosen rank.</li>
</ul>

<p>Because an outer product expands to a full matrix on the fly, the cache only needs the skinny vectors (a_{r}) and (b_{r}).<br />
Per-token memory now reads</p>

<p>[
\boxed{\text{Memory}<em>\text{TPA}=2\,R\,(H+d</em>{h})\,T},\qquad R\ll H.\tag{4}
]</p>

<p>With a LLaMA-style setting ((H=16,\,d_{h}=128,\,R=8)) the key‚Äìvalue slice per token falls from <strong>4096</strong> to <strong>1152</strong> floats‚Äîa <strong>7-to-8√ó</strong> saving that compounds linearly as the context grows.</p>

<h3 id="a-spectrum-not-a-point">A Spectrum, Not a Point</h3>
<p>TPA <strong>unifies</strong> earlier tricks:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Configure the factors like‚Ä¶</th>
      <th style="text-align: left">‚Ä¶and you recover</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">(R=H) and (a_{r}=e_{r})</td>
      <td style="text-align: left">Multi-Head Attention</td>
    </tr>
    <tr>
      <td style="text-align: left">(R=1) and (a_{1}=\mathbf{1})</td>
      <td style="text-align: left">Multi-Query Attention</td>
    </tr>
    <tr>
      <td style="text-align: left">Repeating blocks in (a_{r})</td>
      <td style="text-align: left">Grouped-Query Attention</td>
    </tr>
  </tbody>
</table>

<p>Compression thus becomes a <em>continuous</em> knob (rank <em>R</em>) rather than a handful of hard-wired designs.</p>

<hr />

<h2 id="4position-embeddings-that-stay-low-rank">4‚ÄÇPosition Embeddings That Stay Low-Rank</h2>

<p>Rotary Position Embeddings (RoPE) rotate queries &amp; keys in complex space, enabling length extrapolation.<br />
Earlier low-rank schemes had to keep an uncompressed ‚ÄúRoPE slice.‚Äù<br />
TPA sidesteps that burden through a neat algebraic fact: RoPE acts <strong>block-diagonally</strong> on the outer-product factors, so we can pre-rotate (a_{r}) and (b_{r}) and remain fully factorised‚Äîno hidden KV bloat, no quality loss.</p>

<hr />

<h2 id="5t6-the-first-transformer-built-on-tpa">5‚ÄÇT6: The First Transformer Built on TPA</h2>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/3/34/Transformer%2C_full_architecture.png" alt="Transformer stack" />
<em>Replace the attention sub-layer with TPA blocks and you obtain the <strong>T6 Transformer</strong>.</em></p>

<p>The authors patch TPA blocks into a LLaMA-like architecture (norm-first, SwiGLU MLPs) and coin the result <strong>T6</strong>.<br />
Pre-training on 100 B FineWeb-Edu tokens yields impressive numbers:</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Params</th>
      <th>Context length</th>
      <th>Val. perplexity ‚Üì</th>
      <th>KV memory</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Baseline MHA</td>
      <td>353 M</td>
      <td>8 k</td>
      <td>5.78</td>
      <td>1√ó</td>
    </tr>
    <tr>
      <td><strong>T6 (TPA)</strong></td>
      <td>353 M</td>
      <td>16 k</td>
      <td><strong>5.52</strong></td>
      <td><strong>‚âà 0.12√ó</strong></td>
    </tr>
  </tbody>
</table>

<p>Beyond perplexity, the 353 M T6 edges out MHA, MQA, GQA and MLA across nine zero-shot tasks (ARC, BoolQ, HellaSwag, Winogrande).<br />
Because the KV cache is slimmer, the same A100 carries <strong>65 k-token</strong> contexts at inference with speed on par with MQA.</p>

<hr />

<h2 id="6what-practitioners-can-do-right-now">6‚ÄÇWhat Practitioners Can Do Right Now</h2>

<ul>
  <li><strong>Serve longer prompts on fixed hardware.</strong>‚ÄÉDrop-in swap K &amp; V with rank-8 TPA factors, fine-tune for a few epochs, and unlock ~8√ó context on existing weights.</li>
  <li><strong>Train mid-size models faster.</strong>‚ÄÉFull TPA converges in fewer steps than MHA for the same compute budget.</li>
  <li><strong>Keep the speed.</strong>‚ÄÉTPA integrates with FlashAttention-2 kernels; the fused CUDA op never materialises the full QK‚ä§ matrix, so runtime overhead stays below 2 %.</li>
  <li><strong>Edge and mobile.</strong>‚ÄÉCombine rank-8 TPA with 4-bit quantisation to keep both parameters and cache tiny enough for laptop-class GPUs.</li>
</ul>

<hr />

<h2 id="7limitations--open-questions">7‚ÄÇLimitations &amp; Open Questions</h2>

<p>TPA is powerful‚Äîbut not perfect.<br />
Kernel support is GPU-first; CPU and Metal variants lag a version behind.<br />
Rank selection remains empirical (the paper defaults to (R=8)); auto-tuning heuristics are an open field.<br />
While T6 reaches 65 k tokens cleanly, pushing beyond 100 k may require ALiBi-style biasing or gradient injection tricks.<br />
Finally, modalities beyond text‚Äîvision, audio, reinforcement-learning trajectories‚Äîawait dedicated exploration.</p>

<hr />

<h2 id="8conclusion--a-smarter-coordinate-system-for-attention">8‚ÄÇConclusion ‚Äî A Smarter Coordinate System for Attention</h2>

<p>Tensor Product Attention reframes the long-context dilemma: instead of pruning information, <strong>re-encode</strong> it in a basis where it costs less memory to store.<br />
Because MHA, MQA and GQA appear as limiting cases, TPA offers a principled, tunable bridge between full expressivity and lean deployment budgets.<br />
The accompanying T6 experiments show that you can have your cake (low perplexity, solid downstream scores) and eat it too (longer context, smaller cache, faster convergence).<br />
When future chatbots retain entire books or sprawling codebases inside a single GPU, the factorised elegance of TPA will be a major reason why.</p>

<hr />

<h3 id="reference">Reference</h3>

<p><strong>Jiang et al.</strong> ‚Äì <em>Tensor Product Attention Is All You Need</em> (2025).<br />
<a href="https://arxiv.org/abs/2501.06425">https://arxiv.org/abs/2501.06425</a></p>
</p>

      <a href="/tensorandquarks.github.io/2025/02/13/tpa.html" class="read-more">Read more ‚Üí</a>
    </li>
  
    <li class="post-card">
      <h2><a href="/tensorandquarks.github.io/2025/02/06/capa.html"></a></h2>

      <p class="post-meta">
        February 6, 2025
        
           <span class="inline-tag">ML</span>
        
      </p>

      <p><h1 id="the-hidden-danger-of-ai-oversight-why-model-similarity-might-undermine-reliability"><strong>The Hidden Danger of AI Oversight: Why Model Similarity Might Undermine Reliability</strong></h1>

<p>Artificial Intelligence, particularly Large Language Models (LLMs) like ChatGPT, Llama, and Gemini, has witnessed extraordinary progress. These powerful models can effortlessly handle tasks from writing articles to solving complex reasoning problems. Yet, as these models become smarter, ensuring they‚Äôre behaving as intended is becoming harder for humans alone.</p>

</p>

      <a href="/tensorandquarks.github.io/2025/02/06/capa.html" class="read-more">Read more ‚Üí</a>
    </li>
  
    <li class="post-card">
      <h2><a href="/tensorandquarks.github.io/2025/01/30/warpdrives.html">The Semiclassical Death of Warp Bubbles</a></h2>

      <p class="post-meta">
        January 30, 2025
        
           <span class="inline-tag">Astrophysics</span>
        
      </p>

      <p><h1 id="the-semiclassical-death-of-warp-bubbles">The Semiclassical Death of Warp Bubbles</h1>

<h2 id="introduction-from-sci-fi-to-semi-classical-reality">Introduction: From Sci-Fi to Semi-Classical Reality</h2>

<p>The dream of faster-than-light travel has long danced on the edge of science and imagination. Since Miguel Alcubierre first proposed a warp drive metric in 1994‚Äîa solution to Einstein‚Äôs field equations that allows a spaceship to ‚Äúsurf‚Äù through spacetime by contracting space in front of it and expanding it behind‚Äîscientists have speculated whether such a phenomenon could ever be physically realized.</p>

</p>

      <a href="/tensorandquarks.github.io/2025/01/30/warpdrives.html" class="read-more">Read more ‚Üí</a>
    </li>
  
    <li class="post-card">
      <h2><a href="/tensorandquarks.github.io/2025/01/23/alexnetvsresnet.html">How AlexNet Lit the Spark and ResNet Fanned the Flames</a></h2>

      <p class="post-meta">
        January 23, 2025
        
           <span class="inline-tag">ML</span>
        
      </p>

      <p><h1 id="how-alexnet-lit-the-spark-and-resnet-fanned-the-flames">How AlexNet Lit the Spark and ResNet Fanned the Flames</h1>

<p>In the ever-evolving landscape of deep learning, certain architectures have defined turning points in how neural networks are designed, trained, and understood. Among these, <strong>AlexNet</strong> and <strong>ResNet</strong> stand out as monumental contributions that shifted the paradigm of computer vision and image classification. Though separated by just three years, these two architectures reflect fundamentally different eras of deep learning‚ÄîAlexNet laid the groundwork for deep convolutional networks, while ResNet solved the pressing problems that deeper architectures introduced.</p>

</p>

      <a href="/tensorandquarks.github.io/2025/01/23/alexnetvsresnet.html" class="read-more">Read more ‚Üí</a>
    </li>
  
    <li class="post-card">
      <h2><a href="/tensorandquarks.github.io/2025/01/16/multimessenger.html">Black Hole Meets Neutron Star. Nothing Happens. Everything Changes</a></h2>

      <p class="post-meta">
        January 16, 2025
        
           <span class="inline-tag">Astrophysics</span>
        
      </p>

      <p><h1 id="black-hole-meets-neutron-star-nothing-happens-everything-changes">Black Hole Meets Neutron Star. Nothing Happens. Everything Changes</h1>

<h2 id="introduction-when-gravity-speaks-and-light-doesnt">Introduction: When Gravity Speaks and Light Doesn‚Äôt</h2>

<p>Astronomy entered a new era in 2017 when scientists witnessed the first ever multi-messenger event: GW170817. It was a neutron star collision that didn‚Äôt just ripple space-time but also burst forth in light‚Äîgamma rays, optical waves, X-rays, and more. Since then, the race has been on to catch more of these cosmic spectacles. But what happens when nature offers only silence?</p>

</p>

      <a href="/tensorandquarks.github.io/2025/01/16/multimessenger.html" class="read-more">Read more ‚Üí</a>
    </li>
  
    <li class="post-card">
      <h2><a href="/tensorandquarks.github.io/2025/01/09/braneworld.html">What happens to time when your universe can move through another</a></h2>

      <p class="post-meta">
        January 9, 2025
        
           <span class="inline-tag">Astrophysics</span>
        
      </p>

      <p><h1 id="what-happens-to-time-when-your-universe-can-move-through-another">What happens to time when your universe can move through another?</h1>

<h2 id="introduction-a-universe-that-moves-and-time-that-bends">Introduction: A Universe That Moves, and Time That Bends</h2>

<p>Time travel is a fascinating concept ‚Äî the stuff of science fiction and countless philosophical puzzles. But sometimes, the idea creeps into legitimate physics. Not as a machine or paradox, but as a byproduct of how we define time and causality in the first place. The paper <em>Back to the Future: Causality on a Moving Braneworld</em> ventures into this territory, asking what happens to causality ‚Äî the idea that cause comes before effect ‚Äî when our entire universe isn‚Äôt stationary but moves through a higher-dimensional space.</p>

</p>

      <a href="/tensorandquarks.github.io/2025/01/09/braneworld.html" class="read-more">Read more ‚Üí</a>
    </li>
  
    <li class="post-card">
      <h2><a href="/tensorandquarks.github.io/2025/01/02/cmb.html">Can We See the Shape of the Universe?</a></h2>

      <p class="post-meta">
        January 2, 2025
        
           <span class="inline-tag">Astrophysics</span>
        
      </p>

      <p><h1 id="can-we-see-the-shape-of-the-universe">Can We See the Shape of the Universe?</h1>

<h2 id="1-introduction">1. Introduction</h2>

<p>What is the shape of the universe? Is it infinite or finite but unbounded, like a video game world that wraps around on itself? While general relativity has given us profound insights into the local curvature of spacetime, it leaves unanswered the question of the universe‚Äôs global shape. In her 2001 paper, <em>Topology and the Cosmic Microwave Background</em>, Janna Levin explores how cosmology and topology intersect‚Äîhow the universe‚Äôs large-scale connectivity might be imprinted in the faint glow of the early universe: the cosmic microwave background (CMB). This paper not only bridges mathematics and astrophysics but also pushes the philosophical boundary between what can be known and what must remain an assumption.</p>

</p>

      <a href="/tensorandquarks.github.io/2025/01/02/cmb.html" class="read-more">Read more ‚Üí</a>
    </li>
  
    <li class="post-card">
      <h2><a href="/tensorandquarks.github.io/2024/12/26/colonization.html">Will We Colonise Mars in the Next 50 Years?</a></h2>

      <p class="post-meta">
        December 26, 2024
        
           <span class="inline-tag">Astrophysics</span>
        
      </p>

      <p><h1 id="will-we-colonise-mars-in-the-next-50-years">Will We Colonise Mars in the Next 50 Years?</h1>

<p>This was my assignment for a university coursework module, where I was tasked to evaluate the likelihood of humans colonising Mars within the next 50 years. As someone fascinated by space exploration and future technologies, this topic struck a chord with me. What began as a research task soon turned into a deep dive into the science, speculation, and possibilities surrounding Mars colonisation. Here‚Äôs what I found.</p>

</p>

      <a href="/tensorandquarks.github.io/2024/12/26/colonization.html" class="read-more">Read more ‚Üí</a>
    </li>
  
    <li class="post-card">
      <h2><a href="/tensorandquarks.github.io/2024/12/19/solar-poles.html">Chasing the Sun‚Äôs Secrets: Why the Solar Poles Hold the Key to Our Star's Future</a></h2>

      <p class="post-meta">
        December 19, 2024
        
           <span class="inline-tag">Astrophysics</span>
        
      </p>

      <p><h1 id="exploring-the-solar-poles-unlocking-the-suns-final-frontier">Exploring the Solar Poles: Unlocking the Sun‚Äôs Final Frontier</h1>

<h2 id="introduction-the-last-great-frontier-of-solar-exploration">Introduction: The Last Great Frontier of Solar Exploration</h2>

<p>For centuries, humanity has observed the Sun ‚Äî tracking sunspots, solar flares, and cycles of activity. Telescopes, space observatories, and satellites have offered remarkable insights into our star‚Äôs behavior. Yet, an entire region of the Sun remains practically unexplored: its poles. The paper titled <em>‚ÄúExploring the Solar Poles: The Last Great Frontier of the Sun‚Äù</em> (Nandy et al., 2023) sets out to emphasize just how critical this overlooked region is to understanding the inner workings of our star. The authors argue that the solar poles hold vital clues to the Sun‚Äôs magnetic field generation, its cycle, and the behavior of space weather.</p>

</p>

      <a href="/tensorandquarks.github.io/2024/12/19/solar-poles.html" class="read-more">Read more ‚Üí</a>
    </li>
  
    <li class="post-card">
      <h2><a href="/tensorandquarks.github.io/2024/12/12/AstroML.html">Exploring astroML: Machine Learning Among the Stars</a></h2>

      <p class="post-meta">
        December 12, 2024
        
           <span class="inline-tag">MLAstrophysics</span>
        
      </p>

      <p><h1 id="exploring-astroml-machine-learning-among-the-stars">Exploring astroML: Machine Learning Among the Stars</h1>

<h2 id="the-astronomers-new-toolbox">The Astronomer‚Äôs New Toolbox</h2>

<p>Modern astronomy has evolved into a data-driven science. With massive sky surveys like SDSS (Sloan Digital Sky Survey), Pan-STARRS, and the upcoming LSST producing petabytes of data, traditional approaches no 
longer suffice. Manual inspection and simplistic models simply can‚Äôt scale with this astronomical data deluge. Enter <strong>astroML</strong>, a library that bridges the gap between astronomy and modern machine learning. 
astroML is a Python-based library built on top of familiar scientific computing tools like NumPy, SciPy, matplotlib, and scikit-learn. But what sets it apart is its thoughtful design ‚Äî tailored to real-world 
astronomical problems. From irregular time series to galaxy classification, astroML brings statistically sound and domain-specific tools to the fingertips of astronomers, physicists, and data scientists alike.</p>

</p>

      <a href="/tensorandquarks.github.io/2024/12/12/AstroML.html" class="read-more">Read more ‚Üí</a>
    </li>
  
    <li class="post-card">
      <h2><a href="/tensorandquarks.github.io/2024/12/05/transformers.html">Attention Is All You Need: The Paper That Changed Everything</a></h2>

      <p class="post-meta">
        December 5, 2024
        
           <span class="inline-tag">ML</span>
        
      </p>

      <p><h1 id="attention-is-all-you-need-the-paper-that-changed-everything">Attention Is All You Need: The Paper That Changed Everything</h1>

<p>If you‚Äôve ever interacted with ChatGPT, asked an AI to summarize a document, or translated a phrase using Google Translate, you‚Äôre experiencing the legacy of a paper that redefined modern artificial intelligence. 
Published in 2017 by Vaswani et al., the paper <strong>‚ÄúAttention Is All You Need‚Äù</strong> introduced the world to the <strong>Transformer</strong> architecture. This seemingly simple idea ‚Äî that attention mechanisms alone can model 
complex language patterns without relying on recurrence or convolutions ‚Äî has since become the bedrock of nearly every major NLP system.</p>

</p>

      <a href="/tensorandquarks.github.io/2024/12/05/transformers.html" class="read-more">Read more ‚Üí</a>
    </li>
  
    <li class="post-card">
      <h2><a href="/tensorandquarks.github.io/2024/11/28/neutrinos.html">The Neutrino Mystery of SN 1987A Still Baffles Modern Physics</a></h2>

      <p class="post-meta">
        November 28, 2024
        
           <span class="inline-tag">Astrophysics</span>
        
      </p>

      <p><h2 id="introduction-the-first-second-that-changed-everything">Introduction: The First Second That Changed Everything</h2>

<p>On February 23, 1987, astronomers witnessed something extraordinary. A massive blue supergiant in the Large Magellanic Cloud went supernova. Its light was dazzling, but for physicists, the real treasure arrived 
hours earlier‚Äîin the form of 19 ghostly signals captured by two underground neutrino detectors. This was <strong>Supernova 1987A</strong>, the closest observed supernova in nearly four centuries and the first ever accompanied
by direct neutrino detections. These few dozen elusive particles validated decades of theoretical work in core-collapse physics and marked the beginning of what we now call multi-messenger astronomy.</p>

<p>Yet, more than 35 years later, our best simulations‚Äîarmed with full general relativity, detailed microphysics, and modern computing power‚Äîstill <strong>cannot reproduce what those neutrino detectors saw</strong> in 1987.</p>

</p>

      <a href="/tensorandquarks.github.io/2024/11/28/neutrinos.html" class="read-more">Read more ‚Üí</a>
    </li>
  
    <li class="post-card">
      <h2><a href="/tensorandquarks.github.io/2024/11/21/seal-tools.html">Seal-Tools: Teaching AI Agents to Use Tools Like Developers</a></h2>

      <p class="post-meta">
        November 21, 2024
        
           <span class="inline-tag">ML</span>
        
      </p>

      <p><h1 id="teaching-ai-to-use-tools--the-right-way">Teaching AI to Use Tools ‚Äî The Right Way</h1>
<p><strong>A Deep Dive into Seal-Tools: The Dataset That Makes LLMs Smarter Agents</strong></p>

<p>Imagine asking your AI assistant to ‚Äúbook a flight to Paris, then schedule a taxi to the airport and convert the final bill to Euros.‚Äù Sounds simple, right? In reality, for most AI models, this isn‚Äôt just 
hard ‚Äî it‚Äôs nearly impossible to get right without human babysitting.</p>

<p>That‚Äôs because tool use, chaining functions, and executing multi-step operations <strong>requires structured reasoning</strong>, parameter handling, and format control ‚Äî things even the smartest LLMs struggle with today.</p>

</p>

      <a href="/tensorandquarks.github.io/2024/11/21/seal-tools.html" class="read-more">Read more ‚Üí</a>
    </li>
  
    <li class="post-card">
      <h2><a href="/tensorandquarks.github.io/2024/11/14/tensors.html">What Are Tensors?</a></h2>

      <p class="post-meta">
        November 14, 2024
        
           <span class="inline-tag">MLAstrophysics</span>
        
      </p>

      <p><h1 id="what-are-tensors">What Are Tensors?</h1>

<p>Tensors are fundamental mathematical objects that appear across various domains such as physics, computer science, and engineering. At their core, tensors are multi-dimensional arrays that generalize the 
concepts of scalars (single numbers), vectors (one-dimensional arrays), and matrices (two-dimensional arrays). Unlike simple arrays, tensors are not just containers of numbers‚Äîthey come with transformation 
rules that allow them to describe physical phenomena in a way that remains consistent across coordinate systems.</p>

</p>

      <a href="/tensorandquarks.github.io/2024/11/14/tensors.html" class="read-more">Read more ‚Üí</a>
    </li>
  
    <li class="post-card">
      <h2><a href="/tensorandquarks.github.io/2024/11/07/React.html">From ‚ÄúWhy‚Äù to ‚ÄúHow‚Äù: ReAct‚Äôs Unified Reasoning-Acting Paradigm</a></h2>

      <p class="post-meta">
        November 7, 2024
        
           <span class="inline-tag">ML</span>
        
      </p>

      <p><h1 id="from-why-to-how-reacts-unified-reasoning-acting-paradigm">From ‚ÄúWhy‚Äù to ‚ÄúHow‚Äù: ReAct‚Äôs Unified Reasoning-Acting Paradigm</h1>

<p>Large language models (LLMs) have reshaped natural language processing by demonstrating impressive capabilities in text generation, summarization, and translation. Yet, as powerful as they are, 
these models often struggle when asked to perform complex, multi-step tasks that require deliberate planning and interaction with external information sources. Traditional chain-of-thought (CoT) 
prompting enables LLMs to articulate intermediate reasoning steps, but it remains confined to the model‚Äôs internal knowledge and inference capabilities. Conversely, action-based approaches have allowed 
models to execute external operations‚Äîsuch as querying an API or navigating an environment‚Äîbut lack explicit internal reasoning, leading to unexplainable or brittle behavior. The ReAct framework addresses 
this gap by synergizing reasoning and acting in a unified prompt-based paradigm that interleaves ‚Äúthoughts‚Äù and ‚Äúactions‚Äù to solve complex tasks more effectively and transparently.</p>

</p>

      <a href="/tensorandquarks.github.io/2024/11/07/React.html" class="read-more">Read more ‚Üí</a>
    </li>
  
    <li class="post-card">
      <h2><a href="/tensorandquarks.github.io/2024/10/31/Gap-in-llms.html">From Facts to Insight: Bridging the Compositionality Gap in Language Models</a></h2>

      <p class="post-meta">
        October 31, 2024
        
           <span class="inline-tag">ML</span>
        
      </p>

      <p><h1 id="from-facts-to-insight-bridging-the-compositionality-gap-in-language-models">From Facts to Insight: Bridging the Compositionality Gap in Language Models</h1>

<p>Large language models (LLMs) such as GPT-3 have transformed natural language understanding by memorizing vast amounts of text. Yet, when faced with questions that require <strong>combining</strong> multiple pieces 
of knowledge‚Äîso-called <em>compositional reasoning</em>‚Äîeven the biggest models stumble. In their paper <em>Measuring and Narrowing the Compositionality Gap in Language Models</em>, Press et al. introduce a new metric
for this shortfall, show that it persists despite model scale, and propose practical prompting techniques to close it.</p>

</p>

      <a href="/tensorandquarks.github.io/2024/10/31/Gap-in-llms.html" class="read-more">Read more ‚Üí</a>
    </li>
  
    <li class="post-card">
      <h2><a href="/tensorandquarks.github.io/2024/10/24/LoRA.html">LoRA: A Breakthrough in Efficient Fine-Tuning of Large Language Models</a></h2>

      <p class="post-meta">
        October 24, 2024
        
           <span class="inline-tag">ML</span>
        
      </p>

      <p><h1 id="lora-a-breakthrough-in-efficient-fine-tuning-of-large-language-models">LoRA: A Breakthrough in Efficient Fine-Tuning of Large Language Models</h1>

<p>As large language models (LLMs) like GPT-3, LLaMA, and BERT continue to grow in size and influence, one challenge becomes increasingly apparent: while these models offer exceptional capabilities, 
<strong>adapting them for new tasks remains expensive and resource-intensive</strong>. Fine-tuning a model with billions of parameters typically requires large datasets, massive compute power, 
and hours or even days of training time ‚Äî luxuries not everyone can afford.</p>

</p>

      <a href="/tensorandquarks.github.io/2024/10/24/LoRA.html" class="read-more">Read more ‚Üí</a>
    </li>
  
    <li class="post-card">
      <h2><a href="/tensorandquarks.github.io/2024/10/17/fine-tuning-llms.html">Fine-Tuning Language Models: Welcome to the Nerdy Playground of LLMs</a></h2>

      <p class="post-meta">
        October 17, 2024
        
           <span class="inline-tag">ML</span>
        
      </p>

      <p><h1 id="fine-tuning-language-models-welcome-to-the-nerdy-playground-of-llms">Fine-Tuning Language Models: Welcome to the Nerdy Playground of LLMs</h1>
<p><em>From LoRA to RLHF ‚Äî and all the acronyms in between</em></p>

<p>So, you‚Äôve got your hands on a fancy pre-trained language model. Great. It‚Äôs read more text than any human ever will, speaks in Shakespearean iambic pentameter <em>and</em> Python, and can tell you the capital of Burkina Faso at 3 AM.</p>

</p>

      <a href="/tensorandquarks.github.io/2024/10/17/fine-tuning-llms.html" class="read-more">Read more ‚Üí</a>
    </li>
  
    <li class="post-card">
      <h2><a href="/tensorandquarks.github.io/2024/09/21/welcome.html">Welcome to Tensors &amp; Quarks</a></h2>

      <p class="post-meta">
        September 21, 2024
        
           <span class="inline-tag"></span>
        
      </p>

      <p><p>This is the first post!<br />
Here I‚Äôll share ideas in physics, AI, and their cosmic overlaps.</p>

</p>

      <a href="/tensorandquarks.github.io/2024/09/21/welcome.html" class="read-more">Read more ‚Üí</a>
    </li>
  
</ul>

    </main>
    <footer class="site-footer">
      <div class="container">
        <p>¬© 2025 Rahul Thakur ‚Ä¢ Powered by Jekyll</p>
      </div>
    </footer>
  </body>
</html>
