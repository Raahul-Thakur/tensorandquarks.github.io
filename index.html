<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Home | Tensors & Quarks</title>
    <link rel="stylesheet" href="/tensorandquarks.github.io/assets/style.css" />

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>

    <script>
      function toggleDarkMode() {
        document.body.classList.toggle("dark");
        localStorage.setItem("theme", document.body.classList.contains("dark") ? "dark" : "light");
      }
      window.onload = () => {
        if (localStorage.getItem("theme") === "dark") {
          document.body.classList.add("dark");
        }
      };
    </script>

    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Home | Tensors &amp; Quarks</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Home" />
<meta name="author" content="Rahul Thakur" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Exploring the intersection of physics and machine learning." />
<meta property="og:description" content="Exploring the intersection of physics and machine learning." />
<link rel="canonical" href="https://raahul-thakur.github.io/tensorandquarks.github.io/" />
<meta property="og:url" content="https://raahul-thakur.github.io/tensorandquarks.github.io/" />
<meta property="og:site_name" content="Tensors &amp; Quarks" />
<meta property="og:type" content="website" />
<link rel="next" href="https://raahul-thakur.github.io/tensorandquarks.github.io/page2" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Home" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","author":{"@type":"Person","name":"Rahul Thakur"},"description":"Exploring the intersection of physics and machine learning.","headline":"Home","name":"Tensors &amp; Quarks","url":"https://raahul-thakur.github.io/tensorandquarks.github.io/"}</script>
<!-- End Jekyll SEO tag -->

  </head>

  <body>
    <header class="site-header">
      <div class="container header-flex">
        <div class="branding">
          <img src="/tensorandquarks.github.io/assets/images/bio-photo.jpeg" alt="Rahul" class="profile-pic">
          <h1 class="site-title">Tensors & Quarks</h1>
        </div>
        <nav class="nav">
          <a href="/tensorandquarks.github.io/">Home</a>
          <a href="/tensorandquarks.github.io/about.html">About</a>
          <button onclick="toggleDarkMode()">🌓</button>
        </nav>
      </div>
    </header>

    <main class="container">
      <div class="hero">
        <h1>Welcome to Tensors & Quarks</h1>
        <p>Exploring the cosmos of <strong>Physics</strong> & the depths of <strong>Machine Learning</strong>.</p>
      </div>

      <div class="tag-filter">
        <strong>Filter by Tag:</strong>
        <a href="/tensorandquarks.github.io/tags/ml/">ML</a> |
        <a href="/tensorandquarks.github.io/tags/astrophysics/">Astrophysics</a> |
        <a href="/tensorandquarks.github.io/tags/misc/">Misc</a>
      </div>

      <h2>🧠 Latest Posts</h2>
      <ul class="post-list">
        
          <li class="post-card">
            <h3><a href="/tensorandquarks.github.io/2025/03/28/llm-blackbox-pt1.html"></a></h3>
            <p class="post-meta">
              March 28, 2025
              
                — Tags:
                
                  <span class="inline-tag">ML</span>
                
              
            </p>
            <p><h1 id="-on-the-biology-of-a-large-language-model">🧠 On the Biology of a Large Language Model</h1>
<h2 id="exploring-the-hidden-anatomy-of-claude-35-haiku">Exploring the Hidden Anatomy of Claude 3.5 Haiku</h2>

<p><img src="/assets/images/llm_blackbox/A_2D_digital_diagram_presents_a_simplified_represe.png" alt="Attribution Graph Overview" /></p>

<p>In recent months, interpretability research in AI has taken a leap forward, and Anthropic’s work on attribution graphs stands at the forefront. Their new article, <em>“On the Biology of a Large Language Model,”</em> investigates the internal mechanisms of Claude 3.5 Haiku, a compact yet capable language model released in late 2024. But what makes this work truly groundbreaking is not just what the model does — it’s how the researchers dissect its “thought process” to reveal the complex internal machinery that governs its behavior.</p>

<p>At the heart of this research lies a fascinating new tool: the attribution graph. These graphs are like wiring diagrams that trace how specific outputs arise from individual components inside a model — not unlike how neuroscientists track neural pathways to understand the brain. Instead of seeing the model as an opaque black box, attribution graphs allow us to visualize how neurons, attention heads, and internal features come together to form coherent reasoning, memory, and even self-control circuits.</p>

<hr />

<h2 id="-case-studies-in-model-biology">🧩 Case Studies in Model Biology</h2>

<h3 id="-multi-step-reasoning">🔁 Multi-step Reasoning</h3>
<p><img src="/assets/images/llm_blackbox/A_diagram_in_the_digital_2D_vector_graphic_medium_.png" alt="Multi-step Reasoning Circuit" /></p>

<h3 id="-poetry-planning">📝 Poetry Planning</h3>
<p><img src="/assets/images/llm_blackbox/A_2D_digital_diagram_features_interconnected_nodes.png" alt="Poetry Planning Diagram" /></p>

<h3 id="-multilingual-processing">🌍 Multilingual Processing</h3>
<p><img src="/assets/images/llm_blackbox/A_2D_digital_diagram_features_nodes_representing_b.png" alt="Language Circuit Comparison" /></p>

<h3 id="-arithmetic-modules">➕ Arithmetic Modules</h3>
<p><img src="/assets/images/llm_blackbox/A_diagram_presents_four_horizontal_bar_graphs_stac.png" alt="Arithmetic Circuit Visualization" /></p>

<h3 id="-medical-diagnostics-and-hallucination-risk">🧬 Medical Diagnostics and Hallucination Risk</h3>
<p>But not everything is so neat. In tasks involving medical diagnosis, Claude activates a rich set of features linked to symptoms and conditions. Yet these circuits also highlight one of the core limitations: the potential for hallucinations.</p>

<h3 id="-refusals-and-jailbreaks">🚫 Refusals and Jailbreaks</h3>
<p><img src="/assets/images/llm_blackbox/A_directed_graph_diagram_in_a_digital_medium_illus.png" alt="Refusal Bypass Diagram" /></p>

<h3 id="-chain-of-thought-faithfulness">🧭 Chain-of-Thought Faithfulness</h3>
<p><img src="/assets/images/llm_blackbox/A_pair_of_side-by-side_heatmap_visualizations_titl.png" alt="Faithful vs Unfaithful CoT" /></p>

<hr />

<h2 id="-modular-circuit-insights">🧠 Modular Circuit Insights</h2>
<p><img src="/assets/images/llm_blackbox/A_2D_digital_diagram_depicts_an_attribution_graph_.png" alt="Modular Cortex Diagram" /></p>

<h3 id="-component-specialization-and-task-allocation">🔬 Component Specialization and Task Allocation</h3>
<p><img src="/assets/images/llm_blackbox/head_importance.png" alt="Head Importance Across Tasks" /><br />
<img src="/assets/images/llm_blackbox/component_contribution.png" alt="Attention vs MLP Contribution by Task" /></p>

<hr />

<h2 id="-emergence-of-behavior">📈 Emergence of Behavior</h2>
<p><img src="/assets/images/llm_blackbox/circuit_emergence.png" alt="Circuit Emergence Timeline" /></p>

<h2 id="-token-level-attribution">🧮 Token-Level Attribution</h2>
<p><img src="/assets/images/llm_blackbox/token_attribution.png" alt="Token Attribution Heatmap" /></p>

<hr />

<h2 id="-assessment-why-this-research-matters">🧪 Assessment: Why This Research Matters</h2>

<p>This paper represents a major leap in our journey to truly understand and control the models we build. In an era where large language models are becoming central to everything from education and entertainment to scientific discovery and national security, interpretability is no longer optional — it’s foundational.</p>

<p>Until now, most interpretability techniques have focused on shallow post-hoc explanations. Attribution graphs, in contrast, present a causal and modular view of model computation.</p>

<h3 id="-key-problems-this-research-helps-solve">✅ Key Problems This Research Helps Solve</h3>

<ul>
  <li>Debugging hallucinations and false memory recall</li>
  <li>Understanding misalignment and potential deception</li>
  <li>Tracing harmful outputs to specific circuits</li>
  <li>Improving faithfulness in chain-of-thought prompting</li>
  <li>Isolating reusable components for modular training</li>
  <li>Auditing refusals and jailbreak vulnerabilities</li>
  <li>Interpreting multilingual and multi-task behavior</li>
</ul>

<h3 id="-limitations-of-the-paper">❗ Limitations of the Paper</h3>

<ul>
  <li><strong>Scalability</strong> to larger models like Claude 3.5 Sonnet or GPT-4</li>
  <li><strong>Subjectivity</strong> in interpreting circuit purpose</li>
  <li><strong>No deep dive into training-time emergence</strong></li>
  <li>Risk of <strong>confirmation bias</strong> in attribution analysis</li>
</ul>

<hr />

<h3 id="-attribution-diagnostics">🔎 Attribution Diagnostics</h3>
<p><img src="/assets/images/llm_blackbox/A_2x2_grid_diagram_with_attribution_error_types_is.png" alt="Attribution Error Quadrants" /><br />
<img src="/assets/images/llm_blackbox/A_diagram_in_the_image_illustrates_concepts_relate.png" alt="Stages of Attribution Flow" /></p>

<hr />

<h2 id="-final-reflection">🧠 Final Reflection</h2>

<p>In sum, <em>“On the Biology of a Large Language Model”</em> is not just a clever metaphor — it’s a manifesto for the next era of model interpretability. It doesn’t just show that language models compute — it shows how they think.</p>

<p>And that’s a future worth decoding.</p>

<hr />

<h2 id="-reference">🔗 Reference</h2>
<p>📄 <a href="https://transformer-circuits.pub/2025/attribution-graphs/biology.html">Read the original paper on Anthropic’s official blog</a></p>
</p>
            <a href="/tensorandquarks.github.io/2025/03/28/llm-blackbox-pt1.html" class="read-more">Read more →</a>
          </li>
        
          <li class="post-card">
            <h3><a href="/tensorandquarks.github.io/2025/03/20/inception.html"></a></h3>
            <p class="post-meta">
              March 20, 2025
              
                — Tags:
                
                  <span class="inline-tag">ML</span>
                
              
            </p>
            <p><h1 id="the-deepening-layers-of-inception-a-journey-through-cnn-time">The Deepening Layers of Inception: A Journey Through CNN Time</h1>

<p>The story of the Inception architecture is one of ingenuity, iteration, and elegance in the field of deep learning. At a time when researchers were obsessed with increasing the depth and complexity of convolutional neural networks (CNNs) to improve accuracy on large-scale visual tasks, Google’s research team asked a different question: How can we go deeper without paying the full computational price? The answer was Inception—a family of architectures that offered a bold new design paradigm, prioritizing both <strong>computational efficiency and representational power</strong>. From <strong>Inception v1 (GoogLeNet)</strong> to <strong>Inception-ResNet v2</strong>, each version brought transformative ideas that would ripple throughout the deep learning community. This post unpacks the entire journey, layer by layer, innovation by innovation.</p>

</p>
            <a href="/tensorandquarks.github.io/2025/03/20/inception.html" class="read-more">Read more →</a>
          </li>
        
          <li class="post-card">
            <h3><a href="/tensorandquarks.github.io/2025/03/13/imagenet.html"></a></h3>
            <p class="post-meta">
              March 13, 2025
              
                — Tags:
                
                  <span class="inline-tag">ML</span>
                
              
            </p>
            <p><h1 id="how-imagenet-taught-machines-to-see">How ImageNet Taught Machines to See</h1>

<h2 id="the-vision-behind-the-dataset">The Vision Behind the Dataset</h2>

<p>In the early 2000s, artificial intelligence was still stumbling in the dark when it came to understanding images. Researchers had built systems that could play chess or perform basic language tasks, but when it came to something a toddler could do—like identifying a cat in a photo—machines struggled. There was a glaring gap between the potential of machine learning and its real-world applications in vision.</p>

</p>
            <a href="/tensorandquarks.github.io/2025/03/13/imagenet.html" class="read-more">Read more →</a>
          </li>
        
          <li class="post-card">
            <h3><a href="/tensorandquarks.github.io/2025/03/06/random-transformation.html"></a></h3>
            <p class="post-meta">
              March 6, 2025
              
                — Tags:
                
                  <span class="inline-tag">ML</span>
                
              
            </p>
            <p><h1 id="the-random-illusion-why-adversarial-defenses-arent-as-robust-as-they-seem">The Random Illusion: Why Adversarial Defenses Aren’t as Robust as They Seem</h1>

<p>The field of adversarial machine learning is built on a paradox: models that perform impressively on natural data can be shockingly vulnerable to small, human-imperceptible perturbations. These adversarial examples expose a fragility in deep networks that could have serious consequences in security-critical domains like autonomous driving, medical imaging, or biometric authentication. Naturally, defenses against these attacks have been the subject of intense research. Among them, a seemingly simple strategy has gained popularity: <strong>random transformations</strong>. By applying random, often non-differentiable perturbations to input images—such as resizing, padding, cropping, JPEG compression, or color quantization—these methods hope to break the adversary’s control over the gradients that guide attacks. At first glance, it seems effective. Robust accuracy increases. Attacks fail. But is this robustness genuine?</p>

</p>
            <a href="/tensorandquarks.github.io/2025/03/06/random-transformation.html" class="read-more">Read more →</a>
          </li>
        
          <li class="post-card">
            <h3><a href="/tensorandquarks.github.io/2025/02/27/Polysemanticity.html"></a></h3>
            <p class="post-meta">
              February 27, 2025
              
                — Tags:
                
                  <span class="inline-tag">ML</span>
                
              
            </p>
            <p><h1 id="block-geometry--everything-bagel-neurons-decoding-polysemanticity">Block Geometry &amp; Everything-Bagel Neurons: Decoding Polysemanticity</h1>

<h2 id="when-neurons-speak-in-tongues-why-polysemanticity-demands-a-theory-of-capacity">When Neurons Speak in Tongues: Why Polysemanticity Demands a Theory of Capacity</h2>

<p>Crack open a modern vision or language model and you’ll run into a curious spectacle: the <strong>same</strong> unit flares for “cat ears,” “striped shirts,” <strong>and</strong> “the Eiffel Tower.” This phenomenon—<strong>polysemanticity</strong>—is more than a party trick. It frustrates attribution, muddies interpretability dashboards, and complicates any safety guarantee that relies on isolating <em>the</em> “terrorism neuron” or “privacy-violation neuron.”</p>

</p>
            <a href="/tensorandquarks.github.io/2025/02/27/Polysemanticity.html" class="read-more">Read more →</a>
          </li>
        
      </ul>

      <div class="pagination">
        

        <span>Page 1 of 5</span>

        
          <a href="/tensorandquarks.github.io/page2">Older Posts &rarr;</a>
        
      </div>
    </main>

    <footer class="site-footer">
      <div class="container">
        <p>© 2025 Rahul Thakur • Powered by Jekyll</p>
      </div>
    </footer>
  </body>
</html>
