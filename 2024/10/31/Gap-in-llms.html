<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title> | Tensors & Quarks</title>
    <link rel="stylesheet" href="/tensorandquarks.github.io/assets/style.css" />

    <!-- üåì Dark Mode Script -->
    <script>
      function toggleDarkMode() {
        document.body.classList.toggle("dark");
        localStorage.setItem("theme", document.body.classList.contains("dark") ? "dark" : "light");
      }

      window.onload = () => {
        if (localStorage.getItem("theme") === "dark") {
          document.body.classList.add("dark");
        }
      };
    </script>

    <!-- ‚úÖ MathJax Support -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>

    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Tensors &amp; Quarks | Exploring the intersection of physics and machine learning.</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Tensors &amp; Quarks" />
<meta name="author" content="Rahul Thakur" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="From Facts to Insight: Bridging the Compositionality Gap in Language Models Large language models (LLMs) such as GPT-3 have transformed natural language understanding by memorizing vast amounts of text. Yet, when faced with questions that require combining multiple pieces of knowledge‚Äîso-called compositional reasoning‚Äîeven the biggest models stumble. In their paper Measuring and Narrowing the Compositionality Gap in Language Models, Press et al. introduce a new metric for this shortfall, show that it persists despite model scale, and propose practical prompting techniques to close it." />
<meta property="og:description" content="From Facts to Insight: Bridging the Compositionality Gap in Language Models Large language models (LLMs) such as GPT-3 have transformed natural language understanding by memorizing vast amounts of text. Yet, when faced with questions that require combining multiple pieces of knowledge‚Äîso-called compositional reasoning‚Äîeven the biggest models stumble. In their paper Measuring and Narrowing the Compositionality Gap in Language Models, Press et al. introduce a new metric for this shortfall, show that it persists despite model scale, and propose practical prompting techniques to close it." />
<link rel="canonical" href="https://raahul-thakur.github.io/tensorandquarks.github.io/2024/10/31/Gap-in-llms.html" />
<meta property="og:url" content="https://raahul-thakur.github.io/tensorandquarks.github.io/2024/10/31/Gap-in-llms.html" />
<meta property="og:site_name" content="Tensors &amp; Quarks" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-10-31T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Tensors &amp; Quarks" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Rahul Thakur"},"dateModified":"2024-10-31T00:00:00+00:00","datePublished":"2024-10-31T00:00:00+00:00","description":"From Facts to Insight: Bridging the Compositionality Gap in Language Models Large language models (LLMs) such as GPT-3 have transformed natural language understanding by memorizing vast amounts of text. Yet, when faced with questions that require combining multiple pieces of knowledge‚Äîso-called compositional reasoning‚Äîeven the biggest models stumble. In their paper Measuring and Narrowing the Compositionality Gap in Language Models, Press et al. introduce a new metric for this shortfall, show that it persists despite model scale, and propose practical prompting techniques to close it.","headline":"Tensors &amp; Quarks","mainEntityOfPage":{"@type":"WebPage","@id":"https://raahul-thakur.github.io/tensorandquarks.github.io/2024/10/31/Gap-in-llms.html"},"url":"https://raahul-thakur.github.io/tensorandquarks.github.io/2024/10/31/Gap-in-llms.html"}</script>
<!-- End Jekyll SEO tag -->

  </head>

  <body>
    <header class="site-header">
      <div class="container header-flex">
        <div class="branding">
          <img src="/tensorandquarks.github.io/assets/images/bio-photo.jpeg" alt="Rahul" class="profile-pic">
          <h1 class="site-title">Tensors & Quarks</h1>
        </div>
        <nav class="nav">
          <a href="/tensorandquarks.github.io/">Home</a>
          <a href="/tensorandquarks.github.io/about.html">About</a>
          <button onclick="toggleDarkMode()">üåì</button>
        </nav>
      </div>
    </header>

    <main class="container">
      <h1 id="from-facts-to-insight-bridging-the-compositionality-gap-in-language-models">From Facts to Insight: Bridging the Compositionality Gap in Language Models</h1>

<p>Large language models (LLMs) such as GPT-3 have transformed natural language understanding by memorizing vast amounts of text. Yet, when faced with questions that require <strong>combining</strong> multiple pieces 
of knowledge‚Äîso-called <em>compositional reasoning</em>‚Äîeven the biggest models stumble. In their paper <em>Measuring and Narrowing the Compositionality Gap in Language Models</em>, Press et al. introduce a new metric
for this shortfall, show that it persists despite model scale, and propose practical prompting techniques to close it.</p>

<!--more-->

<hr />

<h2 id="the-compositionality-gap-what-and-why">The Compositionality Gap: What and Why</h2>

<p><strong>Compositional reasoning</strong> asks a model not simply to recall one fact but to stitch together two or more facts in novel ways. For example:</p>

<blockquote>
  <p>‚ÄúWhat is the calling code of the birthplace of Frida Kahlo?‚Äù</p>
</blockquote>

<p>Even if an LLM knows:</p>
<ol>
  <li><em>Frida Kahlo was born in Coyoac√°n.</em></li>
  <li><em>Mexico‚Äôs calling code is +52.</em></li>
</ol>

<p>it may still fail to produce <strong>+52</strong> as the combined answer. To quantify this failure mode, Press et al. define the <strong>compositionality gap</strong>:</p>

<blockquote>
  <p><strong>Compositionality Gap</strong> =<br />
(Number of 2-hop questions answered incorrectly despite both sub-questions being answered correctly)<br />
√∑ (Number of 2-hop questions for which both sub-questions are answered correctly)</p>
</blockquote>

<p>This metric exposes how often an LM ‚Äúknows‚Äù the pieces yet cannot compose them into the final answer.</p>

<hr />

<h2 id="crafting-a-diagnostic-benchmark-compositional-celebrities">Crafting a Diagnostic Benchmark: Compositional Celebrities</h2>

<p>To systematically measure the gap, the authors introduce <strong>Compositional Celebrities (CC)</strong>‚Äîan 8.6 K-question, automatically generated dataset of <em>2-hop</em> queries:</p>

<ol>
  <li><strong>Select a celebrity</strong> (e.g., Justin Bieber).</li>
  <li><strong>Retrieve two facts</strong> about their birth (e.g., birth year = 1994; Masters Tournament champion = Jos√© Mar√≠a Olaz√°bal).</li>
  <li><strong>Combine them</strong> into a compositional query:
    <blockquote>
      <p>‚ÄúWho won the Master‚Äôs Tournament in the year Justin Bieber was born?‚Äù</p>
    </blockquote>
  </li>
</ol>

<p>Key properties of CC:</p>

<ul>
  <li><strong>Unlikely combinations</strong>: The paired facts are common individually, but their conjunction rarely appears in any text corpus.</li>
  <li><strong>Decomposability</strong>: Every question naturally splits into two sub-questions, allowing measurement of memorization vs. composition.</li>
</ul>

<hr />

<h2 id="scaling-alone-doesnt-solve-composition">Scaling Alone Doesn‚Äôt Solve Composition</h2>

<p>A natural hope is that simply <strong>increasing model size</strong> would improve both factual recall and reasoning. Press et al. evaluate the GPT-3 family (Ada ‚Üí Davinci) on CC:</p>

<ul>
  <li><strong>1-hop accuracy</strong> climbs steeply with size.</li>
  <li><strong>2-hop accuracy</strong> improves only marginally.</li>
  <li><strong>Compositionality gap</strong> stays near ~40 % across all sizes (0.35 B ‚Üí 175 B parameters).</li>
</ul>

<p>This reveals that while larger models <strong>memorize</strong> ever more facts, they do <strong>not</strong> proportionally enhance their ability to <strong>compose</strong> learned knowledge.</p>

<hr />

<h2 id="elicitive-prompting-encouraging-thought-in-llms">Elicitive Prompting: Encouraging ‚ÄúThought‚Äù in LLMs</h2>

<p>To tackle this, the authors borrow and extend the idea of prompting LMs to <strong>‚Äúthink aloud.‚Äù</strong> Two key methods emerge:</p>

<h3 id="1-chain-of-thought-cot">1. Chain-of-Thought (CoT)</h3>
<ul>
  <li>The model generates a free-form reasoning trace before the final answer.</li>
  <li>Improves multi-hop accuracy, but can be verbose or hard to parse.</li>
</ul>

<h3 id="2-self-ask-prompting">2. Self-Ask Prompting</h3>
<ul>
  <li>A <strong>structured</strong> approach:
    <ol>
      <li>Model decides if follow-up questions are needed.</li>
      <li>It explicitly emits each <code class="language-plaintext highlighter-rouge">Follow up:</code> sub-question.</li>
      <li>It provides each <code class="language-plaintext highlighter-rouge">Intermediate answer:</code> in turn.</li>
      <li>Finally, it states <code class="language-plaintext highlighter-rouge">So the final answer is:</code> in concise form.</li>
    </ol>
  </li>
  <li>By scaffolding decomposition and retrieval, self-ask dramatically narrows the compositionality gap.</li>
</ul>

<hr />

<h2 id="hybrid-reasoning-plugging-in-a-search-engine">Hybrid Reasoning: Plugging in a Search Engine</h2>

<p>Self-ask‚Äôs clear sub-question boundaries make it trivial to integrate an external search API:</p>

<ol>
  <li><strong>LM</strong> generates <code class="language-plaintext highlighter-rouge">Follow up: When was X born?</code></li>
  <li><strong>System</strong> sends that to a search engine and retrieves ‚Äú1994.‚Äù</li>
  <li><strong>LM</strong> continues, using the fetched answer as if it were its own.</li>
</ol>

<p>This <strong>Self-Ask + Search Engine (SA+SE)</strong> requires <strong>no fine-tuning</strong> or prompt changes‚Äîand yields further accuracy gains (often +10 pp) on CC and complementary benchmarks like 2WikiMultiHopQA, Musique, and Bamboogle.</p>

<hr />

<h2 id="empirical-highlights">Empirical Highlights</h2>

<table>
  <thead>
    <tr>
      <th>Method</th>
      <th style="text-align: right">2-hop Accuracy</th>
      <th style="text-align: right">Compositionality Gap</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Direct Prompt</td>
      <td style="text-align: right">~45%</td>
      <td style="text-align: right">~40%</td>
    </tr>
    <tr>
      <td>Chain-of-Thought</td>
      <td style="text-align: right">~60%</td>
      <td style="text-align: right">~25‚Äì30%</td>
    </tr>
    <tr>
      <td><strong>Self-Ask (ours)</strong></td>
      <td style="text-align: right">~75%</td>
      <td style="text-align: right">~10‚Äì15%</td>
    </tr>
    <tr>
      <td><strong>Self-Ask + Search (ours)</strong></td>
      <td style="text-align: right">~85%</td>
      <td style="text-align: right">~5‚Äì10%</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="implications-for-llm-development">Implications for LLM Development</h2>

<ol>
  <li>
    <p><strong>Diagnostic Clarity</strong><br />
The compositionality gap provides a targeted metric for multi-step reasoning beyond aggregate accuracy.</p>
  </li>
  <li>
    <p><strong>Prompt Engineering Matters</strong><br />
Structured elicitive prompts like self-ask can unlock reasoning abilities that mere scale cannot.</p>
  </li>
  <li>
    <p><strong>Hybrid Systems Are Powerful</strong><br />
Seamlessly combining LMs with external tools (search, databases) can bridge knowledge or reasoning shortfalls without retraining.</p>
  </li>
</ol>

<hr />

<h2 id="future-directions">Future Directions</h2>

<ul>
  <li><strong>Deeper Multi-Hop</strong>: Extend evaluation to 3-, 4-, or higher-order reasoning tasks.</li>
  <li><strong>Multilingual &amp; Multimodal</strong>: Test compositional abilities across languages and modalities (e.g., text + images).</li>
  <li><strong>Architectural Innovations</strong>: Design model architectures that internalize explicit decomposition, rather than relying solely on prompting.</li>
</ul>

<hr />

<p><strong>Conclusion</strong><br />
Press et al.‚Äôs work shines a spotlight on a critical blind spot of today‚Äôs LLMs: the gulf between <em>knowing</em> facts and <em>combining</em> them intelligently. By defining the compositionality gap and demonstrating that structured prompting and smart tool integration can dramatically narrow it, they offer both a diagnostic lens and a practical toolkit for the next generation of reasoning-capable language models.</p>

<hr />

<p>üîó <strong>Read the full paper:</strong><br />
<a href="https://arxiv.org/abs/2210.03350">Measuring and Narrowing the Compositionality Gap in Language Models (arXiv:2210.03350)</a></p>

    </main>

    <footer class="site-footer">
      <div class="container">
        <p>¬© 2025 Rahul Thakur ‚Ä¢ Powered by Jekyll</p>
      </div>
    </footer>
  </body>
</html>
