<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title> | Tensors & Quarks</title>
    <link rel="stylesheet" href="/assets/style.css" />

    <!-- ğŸŒ“ Dark Mode Script -->
    <script>
      function toggleDarkMode() {
        document.body.classList.toggle("dark");
        localStorage.setItem("theme", document.body.classList.contains("dark") ? "dark" : "light");
      }

      window.onload = () => {
        if (localStorage.getItem("theme") === "dark") {
          document.body.classList.add("dark");
        }
      };
    </script>

    <!-- âœ… MathJax Support -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>

    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Tensors &amp; Quarks | Exploring the intersection of physics and machine learning.</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Tensors &amp; Quarks" />
<meta name="author" content="Rahul Thakur" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Fine-Tuning Language Models: Welcome to the Nerdy Playground of LLMs From LoRA to RLHF â€” and all the acronyms in between So, youâ€™ve got your hands on a fancy pre-trained language model. Great. Itâ€™s read more text than any human ever will, speaks in Shakespearean iambic pentameter and Python, and can tell you the capital of Burkina Faso at 3 AM." />
<meta property="og:description" content="Fine-Tuning Language Models: Welcome to the Nerdy Playground of LLMs From LoRA to RLHF â€” and all the acronyms in between So, youâ€™ve got your hands on a fancy pre-trained language model. Great. Itâ€™s read more text than any human ever will, speaks in Shakespearean iambic pentameter and Python, and can tell you the capital of Burkina Faso at 3 AM." />
<link rel="canonical" href="https://www.tensorsandquarks.space/2024/10/17/fine-tuning-llms.html" />
<meta property="og:url" content="https://www.tensorsandquarks.space/2024/10/17/fine-tuning-llms.html" />
<meta property="og:site_name" content="Tensors &amp; Quarks" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-10-17T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Tensors &amp; Quarks" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Rahul Thakur"},"dateModified":"2024-10-17T00:00:00+00:00","datePublished":"2024-10-17T00:00:00+00:00","description":"Fine-Tuning Language Models: Welcome to the Nerdy Playground of LLMs From LoRA to RLHF â€” and all the acronyms in between So, youâ€™ve got your hands on a fancy pre-trained language model. Great. Itâ€™s read more text than any human ever will, speaks in Shakespearean iambic pentameter and Python, and can tell you the capital of Burkina Faso at 3 AM.","headline":"Tensors &amp; Quarks","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.tensorsandquarks.space/2024/10/17/fine-tuning-llms.html"},"url":"https://www.tensorsandquarks.space/2024/10/17/fine-tuning-llms.html"}</script>
<!-- End Jekyll SEO tag -->

  </head>

  <body>
    <header class="site-header">
      <div class="container header-flex">
        <div class="branding">
          <img src="/assets/images/bio-photo.jpeg" alt="Rahul" class="profile-pic">
          <h1 class="site-title">Tensors & Quarks</h1>
        </div>
        <nav class="nav">
          <a href="/">Home</a>
          <a href="/about.html">About</a>
          <button onclick="toggleDarkMode()">ğŸŒ“</button>
        </nav>
      </div>
    </header>

    <main class="container">
      <h1 id="fine-tuning-language-models-welcome-to-the-nerdy-playground-of-llms">Fine-Tuning Language Models: Welcome to the Nerdy Playground of LLMs</h1>
<p><em>From LoRA to RLHF â€” and all the acronyms in between</em></p>

<p>So, youâ€™ve got your hands on a fancy pre-trained language model. Great. Itâ€™s read more text than any human ever will, speaks in Shakespearean iambic pentameter <em>and</em> Python, and can tell you the capital of Burkina Faso at 3 AM.</p>

<!--more-->

<p>But hereâ€™s the catch: it still doesnâ€™t quite â€œgetâ€ <em>you</em>.</p>

<p>Maybe it talks like a robot from the 1800s. Or maybe it knows a bit too much about Reddit but not enough about your customer support tone. Whatever the case, you need it to learn your specific vibe â€” fast.</p>

<p><strong>Enter fine-tuning.</strong></p>

<hr />

<h2 id="wait-what-even-is-fine-tuning">Waitâ€¦ What Even <em>Is</em> Fine-Tuning?</h2>

<p>Fine-tuning is basically the part where you tell the model, <em>â€œHey buddy, I love your general knowledge â€” but now I need you to focus.â€</em> You take that massive, pre-trained, know-it-all and nudge it into becoming an expert on your thing.</p>

<p>That â€œthingâ€ could be:</p>
<ul>
  <li>Writing legal contracts</li>
  <li>Translating Sanskrit to Klingon (donâ€™t judge)</li>
  <li>Summarizing biotech research</li>
  <li>Chatting like your brandâ€™s customer rep who had too much coffee</li>
</ul>

<p>Think of fine-tuning as a crash course in personalization for models. Pretraining gives them their intelligence â€” fine-tuning gives them personality (and hopefully fewer hallucinations).</p>

<hr />

<h2 id="so-why-do-we-need-fine-tuning">Soâ€¦ Why Do We Need Fine-Tuning?</h2>

<p>Good question. Because out-of-the-box LLMs are like really smart interns. They <em>can</em> do the job, but theyâ€™ll probably say something weird in a meeting if you donâ€™t train them a bit.</p>

<p>Without fine-tuning, models:</p>
<ul>
  <li>Hallucinate more often than college students at a Pink Floyd concert</li>
  <li>Struggle with domain-specific language (e.g. legalese, medical jargon, startup buzzwords)</li>
  <li>Sometimes ignore prompts like an emotionally unavailable ex</li>
</ul>

<p>Fine-tuning helps fix that. It teaches models to:</p>
<ul>
  <li>Be accurate (less BS)</li>
  <li>Sound like <em>you</em></li>
  <li>Perform specific tasks better</li>
  <li>Respond faster, more consistently</li>
</ul>

<p>Cool? Cool.</p>

<hr />

<h2 id="the-fine-tuning-taxonomy-aka-choose-your-fighter">The Fine-Tuning Taxonomy (AKA: Choose Your Fighter)</h2>

<p>Fine-tuning isnâ€™t just one thing. Oh no. Itâ€™s a buffet of methods â€” some smart, some elegant, and some so complicated they make your brain sweat. Letâ€™s walk through the big names in the lineup.</p>

<hr />

<h3 id="1-full-fine-tuning">1. Full Fine-Tuning</h3>
<p><em>â€œScrew it, letâ€™s change everything.â€</em></p>

<p>You retrain all the modelâ€™s weights. Thatâ€™s billions of parameters, all shifting like tectonic plates. Itâ€™s powerful but also brutally expensive. Think: GPU farms, late nights, and watching your cloud bill with tears in your eyes.</p>

<p><strong>When to use it:</strong><br />
When youâ€™ve got deep pockets and a very specific goal. Or when youâ€™re a research lab and this <em>is</em> your job.</p>

<hr />

<h3 id="2-sft-supervised-fine-tuning">2. SFT (Supervised Fine-Tuning)</h3>
<p><em>â€œLearn from examples, my young padawan.â€</em></p>

<p>You give the model input-output pairs and say, <em>â€œLook, this is how itâ€™s done.â€</em> Supervised fine-tuning is basically the most wholesome way to teach an LLM.</p>

<p>Great for training models to:</p>
<ul>
  <li>Follow instructions</li>
  <li>Do task-specific stuff like summarizing, translating, or playing therapist</li>
</ul>

<p>Itâ€™s also the base of many other techniques. Kind of like toast. Plain on its own, but essential when things get fancy.</p>

<hr />

<h3 id="3-peft-parameter-efficient-fine-tuning">3. PEFT (Parameter-Efficient Fine-Tuning)</h3>
<p><em>â€œWhy update 100B weights when you can just tweak 0.1% and still look smart?â€</em></p>

<p>This oneâ€™s genius. Instead of rewriting the whole model, you just update a tiny portion. Itâ€™s like teaching someone a new skill without giving them a full brain transplant.</p>

<p>Includes fan favorites like:</p>
<ul>
  <li><strong>LoRA</strong> (Low-Rank Adaptation)</li>
  <li><strong>QLoRA</strong> (LoRA on memory steroids)</li>
  <li><strong>Adapter Tuning</strong></li>
  <li><strong>Prefix Tuning</strong></li>
  <li><strong>IA3</strong>, <strong>BitFit</strong>, and other cool acronym-laden techniques</li>
</ul>

<p>PEFT is perfect if you donâ€™t want your laptop to catch fire during training.</p>

<hr />

<h3 id="4-rlhf-reinforcement-learning-from-human-feedback">4. RLHF (Reinforcement Learning from Human Feedback)</h3>
<p><em>â€œLet the humans decide whatâ€™s good. What could possibly go wrong?â€</em></p>

<p>This is what powers ChatGPTâ€™s polite, non-chaotic responses. After initial training, the model gets judged by humans, and a reward model is built around what people <em>like</em>. Then the LLM learns to optimize for that.</p>

<p>Sounds awesome, butâ€¦ itâ€™s complicated. Requires humans, reward models, reinforcement learning algorithms, a prayer to the compute gods â€” the whole shebang.</p>

<hr />

<h3 id="5-dpo-direct-preference-optimization">5. DPO (Direct Preference Optimization)</h3>
<p><em>â€œLike RLHF, but with fewer breakdowns.â€</em></p>

<p>Instead of all that reinforcement learning jazz, DPO just looks at pairs of responses â€” â€œA is better than Bâ€ â€” and learns from that directly. Itâ€™s cleaner, faster, and less drama. Think of it as the chill cousin of RLHF who actually has their life together.</p>

<hr />

<h3 id="6-instruction-tuning">6. Instruction Tuning</h3>
<p><em>â€œJust tell the model what you want, nicely.â€</em></p>

<p>Instruction tuning is what makes models <em>actually</em> follow prompts like, â€œExplain this to me like Iâ€™m five.â€ You fine-tune on datasets full of natural instructions and desired responses.</p>

<p>Often paired with PEFT or SFT. Models trained this way are much less likely to respond with â€œAs an AI language model, Iâ€¦â€ â€” which, letâ€™s be honest, weâ€™re all tired of.</p>

<hr />

<h3 id="7-multi-task-fine-tuning">7. Multi-Task Fine-Tuning</h3>
<p><em>â€œSummarize this. Translate that. Now solve a riddle.â€</em></p>

<p>Here, you throw all sorts of tasks at the model and let it learn to juggle. Itâ€™s chaotic but surprisingly effective. This kind of training helps models generalize better and makes them solid all-rounders.</p>

<hr />

<h3 id="8-continual-learning--incremental-fine-tuning">8. Continual Learning / Incremental Fine-Tuning</h3>
<p><em>â€œJust one more thingâ€¦â€</em></p>

<p>Fine-tune the model a little more, again and again, without making it forget everything it already learned. Sounds simple, right? Itâ€™s not. Welcome to the world of <strong>catastrophic forgetting</strong> â€” where models suddenly forget how to count after learning to write poetry.</p>

<hr />

<h3 id="9-adapter-tuning">9. Adapter Tuning</h3>
<p><em>â€œPlug in, tune up, unplug. Repeat.â€</em></p>

<p>Instead of changing the model itself, you add tiny trainable â€œadaptersâ€ between layers. Theyâ€™re swappable, reusable, and way more efficient. Think of them like modular power-ups.</p>

<p>Perfect for multi-domain setups â€” swap one adapter for legal stuff, another for memes. Beautiful.</p>

<hr />

<h3 id="10-prompt-tuning--prefix-tuning">10. Prompt Tuning / Prefix Tuning</h3>
<p><em>â€œHack the prompt. Profit.â€</em></p>

<p>Why change the model when you can just <em>influence</em> it? These methods train a custom prompt or prefix that guides the modelâ€™s behavior. Minimal training, maximum effect. Also works great when you want to do quick experiments without going full Frankenstein.</p>

<hr />

<h2 id="final-thoughts-aka-which-one-should-you-pick">Final Thoughts (aka: Which One Should You Pick?)</h2>

<p>Picking a fine-tuning method is like choosing a weapon in a video game â€” depends on your level, your resources, and what kind of monster youâ€™re trying to slay.</p>

<ul>
  <li>Got an RTX 3060 and dreams of training a multilingual poet? Use <strong>QLoRA + SFT + Instruction Tuning</strong>.</li>
  <li>Building a lean offline assistant for your startup? Grab a <strong>GGUF model</strong> for inference and slap on a <strong>LoRA adapter</strong>.</li>
  <li>Want ChatGPT but with your tone and dataset? Start with <strong>SFTT</strong>, then move into <strong>DPO</strong>.</li>
</ul>

<p>In the end, fine-tuning is equal parts science and sorcery. Itâ€™s how we teach these giant models to speak our language â€” and sometimes even our vibe.</p>

<p>So go on. Tinker. Break things. Fine-tune.<br />
The LLM multiverse is just getting started.</p>

<p><strong>â€“ Rahul @ Tensors &amp; Quarks</strong><br />
<em>Still sipping quantized gradients like espresso.</em></p>

    </main>

    <footer class="site-footer">
      <div class="container">
        <p>Â© 2025 Rahul Thakur â€¢ Powered by Jekyll</p>
      </div>
    </footer>
  </body>
</html>
