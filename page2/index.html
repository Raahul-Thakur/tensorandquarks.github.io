<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Home | Tensors & Quarks</title>
    <link rel="stylesheet" href="/tensorandquarks.github.io/assets/style.css" />

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>

    <script>
      function toggleDarkMode() {
        document.body.classList.toggle("dark");
        localStorage.setItem("theme", document.body.classList.contains("dark") ? "dark" : "light");
      }
      window.onload = () => {
        if (localStorage.getItem("theme") === "dark") {
          document.body.classList.add("dark");
        }
      };
    </script>

    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Page 2 of 6 for Home | Tensors &amp; Quarks</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Home" />
<meta name="author" content="Rahul Thakur" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Exploring the intersection of physics and machine learning." />
<meta property="og:description" content="Exploring the intersection of physics and machine learning." />
<link rel="canonical" href="https://raahul-thakur.github.io/tensorandquarks.github.io/page2/" />
<meta property="og:url" content="https://raahul-thakur.github.io/tensorandquarks.github.io/page2/" />
<meta property="og:site_name" content="Tensors &amp; Quarks" />
<meta property="og:type" content="website" />
<link rel="prev" href="https://raahul-thakur.github.io/tensorandquarks.github.io/" />
<link rel="next" href="https://raahul-thakur.github.io/tensorandquarks.github.io/page3" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Home" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Rahul Thakur"},"description":"Exploring the intersection of physics and machine learning.","headline":"Home","url":"https://raahul-thakur.github.io/tensorandquarks.github.io/page2/"}</script>
<!-- End Jekyll SEO tag -->

  </head>

  <body>
    <header class="site-header">
      <div class="container header-flex">
        <div class="branding">
          <img src="/tensorandquarks.github.io/assets/images/bio-photo.jpeg" alt="Rahul" class="profile-pic">
          <h1 class="site-title">Tensors & Quarks</h1>
        </div>
        <nav class="nav">
          <a href="/tensorandquarks.github.io/">Home</a>
          <a href="/tensorandquarks.github.io/about.html">About</a>
          <button onclick="toggleDarkMode()">üåì</button>
        </nav>
      </div>
    </header>

    <main class="container">
      <div class="hero">
        <h1>Welcome to Tensors & Quarks</h1>
        <p>Exploring the cosmos of <strong>Physics</strong> & the depths of <strong>Machine Learning</strong>.</p>
      </div>

      <div class="tag-filter">
        <strong>Filter by Tag:</strong>
        <a href="/tensorandquarks.github.io/tags/ml/">ML</a> |
        <a href="/tensorandquarks.github.io/tags/astrophysics/">Astrophysics</a> |
        <a href="/tensorandquarks.github.io/tags/misc/">Misc</a>
      </div>

      <h2>Latest Posts</h2>
      <ul class="post-list">
        
          <li class="post-card">
            <h3><a href="/tensorandquarks.github.io/2025/03/06/random-transformation.html"></a></h3>
            <p class="post-meta">
              March 6, 2025
              
                ‚Äî Tags:
                
                  <span class="inline-tag">ML</span>
                
              
            </p>
            <p><h1 id="the-random-illusion-why-adversarial-defenses-arent-as-robust-as-they-seem">The Random Illusion: Why Adversarial Defenses Aren‚Äôt as Robust as They Seem</h1>

<p>The field of adversarial machine learning is built on a paradox: models that perform impressively on natural data can be shockingly vulnerable to small, human-imperceptible perturbations. These adversarial examples expose a fragility in deep networks that could have serious consequences in security-critical domains like autonomous driving, medical imaging, or biometric authentication. Naturally, defenses against these attacks have been the subject of intense research. Among them, a seemingly simple strategy has gained popularity: <strong>random transformations</strong>. By applying random, often non-differentiable perturbations to input images‚Äîsuch as resizing, padding, cropping, JPEG compression, or color quantization‚Äîthese methods hope to break the adversary‚Äôs control over the gradients that guide attacks. At first glance, it seems effective. Robust accuracy increases. Attacks fail. But is this robustness genuine?</p>

</p>
            <a href="/tensorandquarks.github.io/2025/03/06/random-transformation.html" class="read-more">Read more ‚Üí</a>
          </li>
        
          <li class="post-card">
            <h3><a href="/tensorandquarks.github.io/2025/02/27/Polysemanticity.html"></a></h3>
            <p class="post-meta">
              February 27, 2025
              
                ‚Äî Tags:
                
                  <span class="inline-tag">ML</span>
                
              
            </p>
            <p><h1 id="block-geometry--everything-bagel-neurons-decoding-polysemanticity">Block Geometry &amp; Everything-Bagel Neurons: Decoding Polysemanticity</h1>

<h2 id="when-neurons-speak-in-tongues-why-polysemanticity-demands-a-theory-of-capacity">When Neurons Speak in Tongues: Why Polysemanticity Demands a Theory of Capacity</h2>

<p>Crack open a modern vision or language model and you‚Äôll run into a curious spectacle: the <strong>same</strong> unit flares for ‚Äúcat ears,‚Äù ‚Äústriped shirts,‚Äù <strong>and</strong> ‚Äúthe Eiffel Tower.‚Äù This phenomenon‚Äî<strong>polysemanticity</strong>‚Äîis more than a party trick. It frustrates attribution, muddies interpretability dashboards, and complicates any safety guarantee that relies on isolating <em>the</em> ‚Äúterrorism neuron‚Äù or ‚Äúprivacy-violation neuron.‚Äù</p>

</p>
            <a href="/tensorandquarks.github.io/2025/02/27/Polysemanticity.html" class="read-more">Read more ‚Üí</a>
          </li>
        
          <li class="post-card">
            <h3><a href="/tensorandquarks.github.io/2025/02/20/wch.html"></a></h3>
            <p class="post-meta">
              February 20, 2025
              
                ‚Äî Tags:
                
                  <span class="inline-tag">Astrophysics</span>
                
              
            </p>
            <p><h1 id="geometry-vs-quantum-damping-two-roads-to-a-smooth-big-bang">Geometry vs Quantum Damping: Two Roads to a Smooth Big Bang</h1>

<p>Imagine rewinding the Universe until every galaxy, atom and photon collapses into a single blinding flash. Is that primal flash a howling chaos or an eerie stillness? In 1979 Roger Penrose wagered on stillness, proposing that the <em>Weyl tensor</em>‚Äîthe slice of curvature that stores tidal distortions and gravitational waves‚Äîwas precisely zero at the Big Bang. Four decades later two very different papers revisit his bet. One rewrites Einstein‚Äôs equations so the <strong>zero-Weyl</strong> state drops out of geometry itself; the other unleashes quantum back-reaction that actively <em>damps</em> any distortion away. Which path makes a smooth dawn more believable?</p>
</p>
            <a href="/tensorandquarks.github.io/2025/02/20/wch.html" class="read-more">Read more ‚Üí</a>
          </li>
        
          <li class="post-card">
            <h3><a href="/tensorandquarks.github.io/2025/02/13/tpa.html"></a></h3>
            <p class="post-meta">
              February 13, 2025
              
                ‚Äî Tags:
                
                  <span class="inline-tag">ML</span>
                
              
            </p>
            <p><h1 id="from-heads-to-factors-a-deep-dive-into-tensor-product-attention-and-the-t6-transformer">From Heads to Factors: A Deep Dive into Tensor Product Attention and the T6 Transformer</h1>

<p><em>A Transformer layer must preserve every key‚Äìvalue pair for every head, layer, and past token‚Äîa memory bill that rises linearly with context length.</em></p>

</p>
            <a href="/tensorandquarks.github.io/2025/02/13/tpa.html" class="read-more">Read more ‚Üí</a>
          </li>
        
          <li class="post-card">
            <h3><a href="/tensorandquarks.github.io/2025/02/06/capa.html"></a></h3>
            <p class="post-meta">
              February 6, 2025
              
                ‚Äî Tags:
                
                  <span class="inline-tag">ML</span>
                
              
            </p>
            <p><h1 id="the-hidden-danger-of-ai-oversight-why-model-similarity-might-undermine-reliability"><strong>The Hidden Danger of AI Oversight: Why Model Similarity Might Undermine Reliability</strong></h1>

<p>Artificial Intelligence, particularly Large Language Models (LLMs) like ChatGPT, Llama, and Gemini, has witnessed extraordinary progress. These powerful models can effortlessly handle tasks from writing articles to solving complex reasoning problems. Yet, as these models become smarter, ensuring they‚Äôre behaving as intended is becoming harder for humans alone.</p>

</p>
            <a href="/tensorandquarks.github.io/2025/02/06/capa.html" class="read-more">Read more ‚Üí</a>
          </li>
        
      </ul>

      <div class="pagination">
        
          <a href="/tensorandquarks.github.io/">&larr; Newer Posts</a>
        

        <span>Page 2 of 6</span>

        
          <a href="/tensorandquarks.github.io/page3">Older Posts &rarr;</a>
        
      </div>
    </main>

    <footer class="site-footer">
      <div class="container">
        <p>¬© 2025 Rahul Thakur ‚Ä¢ Powered by Jekyll</p>
      </div>
    </footer>
  </body>
</html>
