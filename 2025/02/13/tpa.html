<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title> | Tensors & Quarks</title>
    <link rel="stylesheet" href="/tensorandquarks.github.io/assets/style.css" />
    <script>
      // Dark mode script
      function toggleDarkMode() {
        document.body.classList.toggle("dark");
        localStorage.setItem("theme", document.body.classList.contains("dark") ? "dark" : "light");
      }

      window.onload = () => {
        if (localStorage.getItem("theme") === "dark") {
          document.body.classList.add("dark");
        }
      };
    </script>
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Tensors &amp; Quarks | Exploring the intersection of physics and machine learning.</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Tensors &amp; Quarks" />
<meta name="author" content="Rahul Thakur" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="From Heads to Factors: A Deep Dive into Tensor Product Attention and the T6 Transformer A Transformer layer must preserve every key‚Äìvalue pair for every head, layer, and past token‚Äîa memory bill that rises linearly with context length." />
<meta property="og:description" content="From Heads to Factors: A Deep Dive into Tensor Product Attention and the T6 Transformer A Transformer layer must preserve every key‚Äìvalue pair for every head, layer, and past token‚Äîa memory bill that rises linearly with context length." />
<link rel="canonical" href="https://raahul-thakur.github.io/tensorandquarks.github.io/2025/02/13/tpa.html" />
<meta property="og:url" content="https://raahul-thakur.github.io/tensorandquarks.github.io/2025/02/13/tpa.html" />
<meta property="og:site_name" content="Tensors &amp; Quarks" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-02-13T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Tensors &amp; Quarks" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Rahul Thakur"},"dateModified":"2025-02-13T00:00:00+00:00","datePublished":"2025-02-13T00:00:00+00:00","description":"From Heads to Factors: A Deep Dive into Tensor Product Attention and the T6 Transformer A Transformer layer must preserve every key‚Äìvalue pair for every head, layer, and past token‚Äîa memory bill that rises linearly with context length.","headline":"Tensors &amp; Quarks","mainEntityOfPage":{"@type":"WebPage","@id":"https://raahul-thakur.github.io/tensorandquarks.github.io/2025/02/13/tpa.html"},"url":"https://raahul-thakur.github.io/tensorandquarks.github.io/2025/02/13/tpa.html"}</script>
<!-- End Jekyll SEO tag -->

  </head>
  <body>
    <header class="site-header">
      <div class="container header-flex">
        <div class="branding">
          <img src="/tensorandquarks.github.io/assets/images/bio-photo.jpeg" alt="Rahul" class="profile-pic">
          <h1 class="site-title">Tensors & Quarks</h1>
        </div>
        <nav class="nav">
          <a href="/tensorandquarks.github.io/">Home</a>
          <a href="/tensorandquarks.github.io/about.html">About</a>
          <button onclick="toggleDarkMode()">üåì</button>
        </nav>
      </div>
    </header>
    <main class="container">
      <h1 id="from-heads-to-factors-a-deep-dive-into-tensor-product-attention-and-the-t6-transformer">From Heads to Factors: A Deep Dive into Tensor Product Attention and the T6 Transformer</h1>

<p><em>A Transformer layer must preserve every key‚Äìvalue pair for every head, layer, and past token‚Äîa memory bill that rises linearly with context length.</em></p>

<!--more-->

<p><img src="https://upload.wikimedia.org/wikipedia/commons/7/74/Encoder_self-attention%2C_detailed_diagram.png" alt="Self-attention overview" /></p>

<hr />

<h2 id="1why-the-kv-cache-became-public-enemy-1">1‚ÄÇWhy the KV Cache Became Public Enemy #1</h2>

<p>Modern language models rarely choke on parameter counts; they choke on <strong>context</strong>.<br />
During autoregressive generation the model must keep <strong>two</strong> tensors‚Äîkeys and values‚Äîfor <strong>every</strong> past token so that future queries can look back.<br />
With <em>H</em> heads of size <em>d<sub>h</sub></em> and a sequence of <em>T</em> tokens, the cached footprint for a single layer is</p>

<p>[
\boxed{\text{Memory}<em>\text{MHA}=2\,H\,d</em>{h}\,T}\tag{1}
]</p>

<p>On an A100 GPU you can exhaust 40 GB of RAM long before you fill the compute budget, forcing practitioners to trim prompts, chop documents, or shard the KV cache across devices.<br />
Previous fixes (Multi-Query, Grouped-Query, Multi-head Latent Attention) either share keys between heads or summarise them into latents, clawing back memory at the price of modelling fidelity. <strong>Tensor Product Attention (TPA)</strong>‚Äîintroduced in <em>‚ÄúTensor Product Attention Is All You Need‚Äù</em> (2025)‚Äîoffers a radical alternative: <strong>compress</strong> each Q, K, V with a low-rank tensor factorisation so you keep almost all capacity while paying a fraction of the memory bill.</p>

<hr />

<h2 id="2a-one-equation-recap-of-multi-head-attention">2‚ÄÇA One-Equation Recap of Multi-Head Attention</h2>

<p>For a single head the scaled dot-product attention is</p>

<p>[
\mathrm{Attn}(Q,K,V)=\operatorname{softmax}!\Bigl(\tfrac{QK^{\top}}{\sqrt{d_{h}}}\Bigr)V.\tag{2}
]</p>

<p>Multi-Head Attention (MHA) simply repeats that calculation <em>H</em> times with separate projections.<br />
Expressive? Absolutely.<br />
But each head carries its own key‚Äìvalue history, so Equation (1) still looms large in memory-bound deployments.</p>

<hr />

<h2 id="3tensor-product-attention--compress-keys-keep-power">3‚ÄÇTensor Product Attention ‚Äî Compress Keys, Keep Power</h2>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/2/26/AM_diagrams_outer_product.svg" alt="Outer-product factorisation" />
<em>A rank-1 outer product; TPA stores a <strong>sum</strong> of such products rather than a full matrix.</em></p>

<p>The core idea is tantalisingly simple.<br />
Instead of caching a full <em>H √ó d<sub>h</sub></em> key matrix for every token, TPA writes it as a <strong>low-rank sum of outer products</strong></p>

<p>[
K=\sum_{r=1}^{R} a_{r}\;\otimes\;b_{r}^{\top},\tag{3}
]</p>

<p>where</p>

<ul>
  <li>(a_{r}\in\mathbb{R}^{H}) are <strong>head-factors</strong> that vary across heads but not token features,</li>
  <li>(b_{r}\in\mathbb{R}^{d_{h}}) are <strong>token-factors</strong> that vary across features but not heads,</li>
  <li>(R) ‚â™ (H) is the chosen rank.</li>
</ul>

<p>Because an outer product expands to a full matrix on the fly, the cache only needs the skinny vectors (a_{r}) and (b_{r}).<br />
Per-token memory now reads</p>

<p>[
\boxed{\text{Memory}<em>\text{TPA}=2\,R\,(H+d</em>{h})\,T},\qquad R\ll H.\tag{4}
]</p>

<p>With a LLaMA-style setting ((H=16,\,d_{h}=128,\,R=8)) the key‚Äìvalue slice per token falls from <strong>4096</strong> to <strong>1152</strong> floats‚Äîa <strong>7-to-8√ó</strong> saving that compounds linearly as the context grows.</p>

<h3 id="a-spectrum-not-a-point">A Spectrum, Not a Point</h3>
<p>TPA <strong>unifies</strong> earlier tricks:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Configure the factors like‚Ä¶</th>
      <th style="text-align: left">‚Ä¶and you recover</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">(R=H) and (a_{r}=e_{r})</td>
      <td style="text-align: left">Multi-Head Attention</td>
    </tr>
    <tr>
      <td style="text-align: left">(R=1) and (a_{1}=\mathbf{1})</td>
      <td style="text-align: left">Multi-Query Attention</td>
    </tr>
    <tr>
      <td style="text-align: left">Repeating blocks in (a_{r})</td>
      <td style="text-align: left">Grouped-Query Attention</td>
    </tr>
  </tbody>
</table>

<p>Compression thus becomes a <em>continuous</em> knob (rank <em>R</em>) rather than a handful of hard-wired designs.</p>

<hr />

<h2 id="4position-embeddings-that-stay-low-rank">4‚ÄÇPosition Embeddings That Stay Low-Rank</h2>

<p>Rotary Position Embeddings (RoPE) rotate queries &amp; keys in complex space, enabling length extrapolation.<br />
Earlier low-rank schemes had to keep an uncompressed ‚ÄúRoPE slice.‚Äù<br />
TPA sidesteps that burden through a neat algebraic fact: RoPE acts <strong>block-diagonally</strong> on the outer-product factors, so we can pre-rotate (a_{r}) and (b_{r}) and remain fully factorised‚Äîno hidden KV bloat, no quality loss.</p>

<hr />

<h2 id="5t6-the-first-transformer-built-on-tpa">5‚ÄÇT6: The First Transformer Built on TPA</h2>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/3/34/Transformer%2C_full_architecture.png" alt="Transformer stack" />
<em>Replace the attention sub-layer with TPA blocks and you obtain the <strong>T6 Transformer</strong>.</em></p>

<p>The authors patch TPA blocks into a LLaMA-like architecture (norm-first, SwiGLU MLPs) and coin the result <strong>T6</strong>.<br />
Pre-training on 100 B FineWeb-Edu tokens yields impressive numbers:</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Params</th>
      <th>Context length</th>
      <th>Val. perplexity ‚Üì</th>
      <th>KV memory</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Baseline MHA</td>
      <td>353 M</td>
      <td>8 k</td>
      <td>5.78</td>
      <td>1√ó</td>
    </tr>
    <tr>
      <td><strong>T6 (TPA)</strong></td>
      <td>353 M</td>
      <td>16 k</td>
      <td><strong>5.52</strong></td>
      <td><strong>‚âà 0.12√ó</strong></td>
    </tr>
  </tbody>
</table>

<p>Beyond perplexity, the 353 M T6 edges out MHA, MQA, GQA and MLA across nine zero-shot tasks (ARC, BoolQ, HellaSwag, Winogrande).<br />
Because the KV cache is slimmer, the same A100 carries <strong>65 k-token</strong> contexts at inference with speed on par with MQA.</p>

<hr />

<h2 id="6what-practitioners-can-do-right-now">6‚ÄÇWhat Practitioners Can Do Right Now</h2>

<ul>
  <li><strong>Serve longer prompts on fixed hardware.</strong>‚ÄÉDrop-in swap K &amp; V with rank-8 TPA factors, fine-tune for a few epochs, and unlock ~8√ó context on existing weights.</li>
  <li><strong>Train mid-size models faster.</strong>‚ÄÉFull TPA converges in fewer steps than MHA for the same compute budget.</li>
  <li><strong>Keep the speed.</strong>‚ÄÉTPA integrates with FlashAttention-2 kernels; the fused CUDA op never materialises the full QK‚ä§ matrix, so runtime overhead stays below 2 %.</li>
  <li><strong>Edge and mobile.</strong>‚ÄÉCombine rank-8 TPA with 4-bit quantisation to keep both parameters and cache tiny enough for laptop-class GPUs.</li>
</ul>

<hr />

<h2 id="7limitations--open-questions">7‚ÄÇLimitations &amp; Open Questions</h2>

<p>TPA is powerful‚Äîbut not perfect.<br />
Kernel support is GPU-first; CPU and Metal variants lag a version behind.<br />
Rank selection remains empirical (the paper defaults to (R=8)); auto-tuning heuristics are an open field.<br />
While T6 reaches 65 k tokens cleanly, pushing beyond 100 k may require ALiBi-style biasing or gradient injection tricks.<br />
Finally, modalities beyond text‚Äîvision, audio, reinforcement-learning trajectories‚Äîawait dedicated exploration.</p>

<hr />

<h2 id="8conclusion--a-smarter-coordinate-system-for-attention">8‚ÄÇConclusion ‚Äî A Smarter Coordinate System for Attention</h2>

<p>Tensor Product Attention reframes the long-context dilemma: instead of pruning information, <strong>re-encode</strong> it in a basis where it costs less memory to store.<br />
Because MHA, MQA and GQA appear as limiting cases, TPA offers a principled, tunable bridge between full expressivity and lean deployment budgets.<br />
The accompanying T6 experiments show that you can have your cake (low perplexity, solid downstream scores) and eat it too (longer context, smaller cache, faster convergence).<br />
When future chatbots retain entire books or sprawling codebases inside a single GPU, the factorised elegance of TPA will be a major reason why.</p>

<hr />

<h3 id="reference">Reference</h3>

<p><strong>Jiang et al.</strong> ‚Äì <em>Tensor Product Attention Is All You Need</em> (2025).<br />
<a href="https://arxiv.org/abs/2501.06425">https://arxiv.org/abs/2501.06425</a></p>

    </main>
    <footer class="site-footer">
      <div class="container">
        <p>¬© 2025 Rahul Thakur ‚Ä¢ Powered by Jekyll</p>
      </div>
    </footer>
  </body>
</html>
