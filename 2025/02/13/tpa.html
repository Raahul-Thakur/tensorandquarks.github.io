<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title> | Tensors & Quarks</title>
    <link rel="stylesheet" href="/assets/style.css" />

    <!-- üåì Dark Mode Script -->
    <script>
      function toggleDarkMode() {
        document.body.classList.toggle("dark");
        localStorage.setItem("theme", document.body.classList.contains("dark") ? "dark" : "light");
      }

      window.onload = () => {
        if (localStorage.getItem("theme") === "dark") {
          document.body.classList.add("dark");
        }
      };
    </script>

    <!-- ‚úÖ MathJax Configuration and Script -->
    <script>
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$','$$'], ['\\[', '\\]']],
          processEscapes: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      };
    </script>
    <script id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>

    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Tensors &amp; Quarks | Exploring the intersection of physics and machine learning.</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Tensors &amp; Quarks" />
<meta name="author" content="Rahul Thakur" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="From Heads to Factors: A Deep Dive into Tensor Product Attention and the T6 Transformer A Transformer layer must preserve every key‚Äìvalue pair for every head, layer, and past token‚Äîa memory bill that rises linearly with context length." />
<meta property="og:description" content="From Heads to Factors: A Deep Dive into Tensor Product Attention and the T6 Transformer A Transformer layer must preserve every key‚Äìvalue pair for every head, layer, and past token‚Äîa memory bill that rises linearly with context length." />
<link rel="canonical" href="https://www.tensorsandquarks.space/2025/02/13/tpa.html" />
<meta property="og:url" content="https://www.tensorsandquarks.space/2025/02/13/tpa.html" />
<meta property="og:site_name" content="Tensors &amp; Quarks" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-02-13T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Tensors &amp; Quarks" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Rahul Thakur"},"dateModified":"2025-02-13T00:00:00+00:00","datePublished":"2025-02-13T00:00:00+00:00","description":"From Heads to Factors: A Deep Dive into Tensor Product Attention and the T6 Transformer A Transformer layer must preserve every key‚Äìvalue pair for every head, layer, and past token‚Äîa memory bill that rises linearly with context length.","headline":"Tensors &amp; Quarks","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.tensorsandquarks.space/2025/02/13/tpa.html"},"url":"https://www.tensorsandquarks.space/2025/02/13/tpa.html"}</script>
<!-- End Jekyll SEO tag -->

  </head>

  <body>
    <header class="site-header">
      <div class="container header-flex">
        <div class="branding">
          <img src="/assets/images/bio-photo.jpeg" alt="Rahul" class="profile-pic">
          <h1 class="site-title">Tensors & Quarks</h1>
        </div>
        <nav class="nav">
          <a href="/">Home</a>
          <a href="/about.html">About</a>
          <button onclick="toggleDarkMode()">üåì</button>
        </nav>
      </div>
    </header>

    <main class="container">
      <h1 id="from-heads-to-factors-a-deep-dive-into-tensor-product-attention-and-the-t6-transformer">From Heads to Factors: A Deep Dive into Tensor Product Attention and the T6 Transformer</h1>

<p><em>A Transformer layer must preserve every key‚Äìvalue pair for every head, layer, and past token‚Äîa memory bill that rises linearly with context length.</em></p>

<!--more-->

<p align="center">
  <img src="https://upload.wikimedia.org/wikipedia/commons/7/74/Encoder_self-attention%2C_detailed_diagram.png" alt="Self-attention overview" width="600" />
</p>

<h2 id="1why-the-kv-cache-became-public-enemy-1">1‚ÄÇWhy the KV Cache Became Public Enemy #1</h2>

<p>Modern language models rarely choke on parameter counts; they choke on <strong>context</strong>.<br />
During autoregressive generation the model must keep <strong>two</strong> tensors‚Äîkeys and values‚Äîfor <strong>every</strong> past token so that future queries can look back.<br />
With <em>H</em> heads of size <em>d<sub>h</sub></em> and a sequence of <em>T</em> tokens, the cached footprint for <em>one</em> layer is</p>

\[\boxed{\text{Memory}_{\text{MHA}} = 2\,H\,d_{h}\,T}
\tag{1}\]

<p>On an A100 GPU you can exhaust 40 GB of RAM long before you fill the compute budget.<br />
Previous fixes (Multi-Query, Grouped-Query, Multi-head Latent) claw back memory by <strong>sharing</strong> or <strong>summarising</strong> keys, but at some cost in modelling power.<br />
<strong>Tensor Product Attention (TPA)</strong>‚Äîintroduced in <em>‚ÄúTensor Product Attention Is All You Need‚Äù</em> (2025)‚Äîcompresses each Q, K &amp; V with a low-rank factorisation, keeping almost all capacity while paying only a fraction of the memory bill.</p>

<hr />

<h2 id="2a-one-equation-recap-of-multi-head-attention">2‚ÄÇA One-Equation Recap of Multi-Head Attention</h2>

<p>For a single head the scaled-dot-product attention is</p>

\[\mathrm{Attn}(Q,K,V)
  \;=\;
  \operatorname{softmax}\!\left(\frac{QK^{\!\top}}{\sqrt{d_{h}}}\right)V .
\tag{2}\]

<p>Multi-Head Attention (MHA) just repeats that calculation <em>H</em> times with independent projections‚Äîexpressive, but memory-hungry via Equation (1).</p>

<hr />

<h2 id="3tensor-product-attention--compress-keys-keep-power">3‚ÄÇTensor Product Attention ‚Äî Compress Keys, Keep Power</h2>

<p align="center">
  <img src="https://upload.wikimedia.org/wikipedia/commons/2/26/AM_diagrams_outer_product.svg" alt="Outer-product factorisation" width="400" />
</p>
<p><em>A rank-1 outer product; TPA stores a <strong>sum</strong> of such products rather than a full matrix.</em></p>

<p>Instead of caching a full <em>H √ó d<sub>h</sub></em> key matrix for every token, TPA writes it as a <strong>low-rank sum of outer products</strong></p>

\[K \;=\; \sum_{r=1}^{R} a_{r}\;\otimes\;b_{r}^{\!\top},
\tag{3}\]

<p>where</p>

<ul>
  <li>(a_{r}\in\mathbb{R}^{H}) are <strong>head-factors</strong>,</li>
  <li>(b_{r}\in\mathbb{R}^{d_{h}}) are <strong>token-factors</strong>,</li>
  <li>(R \ll H) is the chosen rank.</li>
</ul>

<p>The cache now needs only the skinny vectors (a_{r}, b_{r}), so per-token memory becomes</p>

\[\boxed{\text{Memory}_{\text{TPA}} = 2\,R\,(H+d_{h})\,T},
\qquad R \ll H .
\tag{4}\]

<p>With LLaMA-style numbers ((H{=}16, d_{h}{=}128, R{=}8)) the key-value slice shrinks ‚âà 8 √ó.</p>

<h3 id="a-spectrum-not-a-point">A Spectrum, Not a Point</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Configure the factors like‚Ä¶</th>
      <th style="text-align: left">‚Ä¶and you recover</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">(R=H,\;a_{r}=e_{r})</td>
      <td style="text-align: left">Multi-Head Attention</td>
    </tr>
    <tr>
      <td style="text-align: left">(R=1,\;a_{1}=\mathbf{1})</td>
      <td style="text-align: left">Multi-Query Attention</td>
    </tr>
    <tr>
      <td style="text-align: left">Repeating blocks in (a_{r})</td>
      <td style="text-align: left">Grouped-Query Attention</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="4position-embeddings-that-stay-low-rank">4‚ÄÇPosition Embeddings That Stay Low-Rank</h2>

<p>Rotary Position Embeddings (RoPE) rotate queries &amp; keys in complex space for unlimited extrapolation.<br />
Because the rotation is block-diagonal, we can apply it directly to each factor in Equation (3), so TPA stays compressed‚Äîunlike earlier low-rank schemes that needed an uncompressed ‚ÄúRoPE slice.‚Äù</p>

<hr />

<h2 id="5t6-the-first-transformer-built-on-tpa">5‚ÄÇT6: The First Transformer Built on TPA</h2>

<p align="center">
  <img src="https://upload.wikimedia.org/wikipedia/commons/3/34/Transformer%2C_full_architecture.png" alt="Transformer stack" width="450" />
</p>

<p>The authors swap TPA blocks into a LLaMA-like architecture (norm-first, SwiGLU MLPs) and call the model <strong>T6</strong>.</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Params</th>
      <th>Context length</th>
      <th>Val. ppl ‚Üì</th>
      <th>KV memory</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Baseline MHA</td>
      <td>353 M</td>
      <td>8 k</td>
      <td>5.78</td>
      <td>1 √ó</td>
    </tr>
    <tr>
      <td><strong>T6 (TPA)</strong></td>
      <td>353 M</td>
      <td>16 k</td>
      <td><strong>5.52</strong></td>
      <td><strong>‚âà 0.12 √ó</strong></td>
    </tr>
  </tbody>
</table>

<p>Zero-shot evaluation (ARC, BoolQ, HellaSwag‚Ä¶) shows consistent +1‚Äì2 % accuracy over MHA, MQA, GQA and MLA.<br />
Because the KV cache is slimmer, the same A100 now handles <strong>65 k-token</strong> contexts at inference with speed almost identical to MQA.</p>

<hr />

<h2 id="6what-practitioners-can-do-right-now">6‚ÄÇWhat Practitioners Can Do Right Now</h2>

<ul>
  <li><strong>Serve longer prompts</strong> on fixed hardware ‚Üí factorise K &amp; V (rank 8), quick fine-tune, context ‚âà 8 √ó longer.</li>
  <li><strong>Train mid-size models</strong> faster ‚Üí full TPA converges in fewer steps than MHA for the same FLOPs.</li>
  <li><strong>Edge/mobile</strong> ‚Üí pair rank-8 TPA with 4-bit weight quantisation; both params <em>and</em> cache fit on laptop-class GPUs.</li>
</ul>

<hr />

<h2 id="7limitations--open-questions">7‚ÄÇLimitations &amp; Open Questions</h2>

<ul>
  <li>GPU kernels are ready; CPU/Metal back-ends lag.</li>
  <li>Rank (R) is empirical (paper uses 8); auto-tuning is open research.</li>
  <li>Ultra-long contexts (&gt;100 k) may need ALiBi or gradient-injection tricks.</li>
  <li>Vision/audio Transformers still untested with TPA.</li>
</ul>

<hr />

<h2 id="8conclusion--a-smarter-coordinate-system-for-attention">8‚ÄÇConclusion ‚Äî A Smarter Coordinate System for Attention</h2>

<p>Tensor Product Attention reframes the long-context dilemma: <strong>re-encode</strong> information in a basis where it‚Äôs cheaper to store.<br />
Because MHA, MQA and GQA are limiting cases, TPA offers a tunable continuum rather than discrete compromises.<br />
The T6 experiments show you can have low perplexity <em>and</em> huge context windows <em>and</em> small caches‚Äîall on today‚Äôs hardware.</p>

<hr />

<h3 id="reference">Reference</h3>

<p><strong>Jiang et al.</strong> ¬†‚Äî¬† <em>Tensor Product Attention Is All You Need</em> (2025).<br />
<a href="https://arxiv.org/abs/2501.06425">https://arxiv.org/abs/2501.06425</a></p>

    </main>

    <footer class="site-footer">
      <div class="container">
        <p>¬© 2025 Rahul Thakur ‚Ä¢ Powered by Jekyll</p>
      </div>
    </footer>
  </body>
</html>
