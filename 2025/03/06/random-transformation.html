<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title> | Tensors & Quarks</title>
    <link rel="stylesheet" href="/tensorandquarks.github.io/assets/style.css" />
    <script>
      // Dark mode script
      function toggleDarkMode() {
        document.body.classList.toggle("dark");
        localStorage.setItem("theme", document.body.classList.contains("dark") ? "dark" : "light");
      }

      window.onload = () => {
        if (localStorage.getItem("theme") === "dark") {
          document.body.classList.add("dark");
        }
      };
    </script>
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Tensors &amp; Quarks | Exploring the intersection of physics and machine learning.</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Tensors &amp; Quarks" />
<meta name="author" content="Rahul Thakur" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The Random Illusion: Why Adversarial Defenses Aren‚Äôt as Robust as They Seem The field of adversarial machine learning is built on a paradox: models that perform impressively on natural data can be shockingly vulnerable to small, human-imperceptible perturbations. These adversarial examples expose a fragility in deep networks that could have serious consequences in security-critical domains like autonomous driving, medical imaging, or biometric authentication. Naturally, defenses against these attacks have been the subject of intense research. Among them, a seemingly simple strategy has gained popularity: random transformations. By applying random, often non-differentiable perturbations to input images‚Äîsuch as resizing, padding, cropping, JPEG compression, or color quantization‚Äîthese methods hope to break the adversary‚Äôs control over the gradients that guide attacks. At first glance, it seems effective. Robust accuracy increases. Attacks fail. But is this robustness genuine?" />
<meta property="og:description" content="The Random Illusion: Why Adversarial Defenses Aren‚Äôt as Robust as They Seem The field of adversarial machine learning is built on a paradox: models that perform impressively on natural data can be shockingly vulnerable to small, human-imperceptible perturbations. These adversarial examples expose a fragility in deep networks that could have serious consequences in security-critical domains like autonomous driving, medical imaging, or biometric authentication. Naturally, defenses against these attacks have been the subject of intense research. Among them, a seemingly simple strategy has gained popularity: random transformations. By applying random, often non-differentiable perturbations to input images‚Äîsuch as resizing, padding, cropping, JPEG compression, or color quantization‚Äîthese methods hope to break the adversary‚Äôs control over the gradients that guide attacks. At first glance, it seems effective. Robust accuracy increases. Attacks fail. But is this robustness genuine?" />
<link rel="canonical" href="https://raahul-thakur.github.io/tensorandquarks.github.io/2025/03/06/random-transformation.html" />
<meta property="og:url" content="https://raahul-thakur.github.io/tensorandquarks.github.io/2025/03/06/random-transformation.html" />
<meta property="og:site_name" content="Tensors &amp; Quarks" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-03-06T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Tensors &amp; Quarks" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Rahul Thakur"},"dateModified":"2025-03-06T00:00:00+00:00","datePublished":"2025-03-06T00:00:00+00:00","description":"The Random Illusion: Why Adversarial Defenses Aren‚Äôt as Robust as They Seem The field of adversarial machine learning is built on a paradox: models that perform impressively on natural data can be shockingly vulnerable to small, human-imperceptible perturbations. These adversarial examples expose a fragility in deep networks that could have serious consequences in security-critical domains like autonomous driving, medical imaging, or biometric authentication. Naturally, defenses against these attacks have been the subject of intense research. Among them, a seemingly simple strategy has gained popularity: random transformations. By applying random, often non-differentiable perturbations to input images‚Äîsuch as resizing, padding, cropping, JPEG compression, or color quantization‚Äîthese methods hope to break the adversary‚Äôs control over the gradients that guide attacks. At first glance, it seems effective. Robust accuracy increases. Attacks fail. But is this robustness genuine?","headline":"Tensors &amp; Quarks","mainEntityOfPage":{"@type":"WebPage","@id":"https://raahul-thakur.github.io/tensorandquarks.github.io/2025/03/06/random-transformation.html"},"url":"https://raahul-thakur.github.io/tensorandquarks.github.io/2025/03/06/random-transformation.html"}</script>
<!-- End Jekyll SEO tag -->

  </head>
  <body>
    <header class="site-header">
      <div class="container header-flex">
        <div class="branding">
          <img src="/tensorandquarks.github.io/assets/images/bio-photo.jpeg" alt="Rahul" class="profile-pic">
          <h1 class="site-title">Tensors & Quarks</h1>
        </div>
        <nav class="nav">
          <a href="/tensorandquarks.github.io/">Home</a>
          <a href="/tensorandquarks.github.io/about.html">About</a>
          <button onclick="toggleDarkMode()">üåì</button>
        </nav>
      </div>
    </header>
    <main class="container">
      <h1 id="the-random-illusion-why-adversarial-defenses-arent-as-robust-as-they-seem">The Random Illusion: Why Adversarial Defenses Aren‚Äôt as Robust as They Seem</h1>

<p>The field of adversarial machine learning is built on a paradox: models that perform impressively on natural data can be shockingly vulnerable to small, human-imperceptible perturbations. These adversarial examples expose a fragility in deep networks that could have serious consequences in security-critical domains like autonomous driving, medical imaging, or biometric authentication. Naturally, defenses against these attacks have been the subject of intense research. Among them, a seemingly simple strategy has gained popularity: <strong>random transformations</strong>. By applying random, often non-differentiable perturbations to input images‚Äîsuch as resizing, padding, cropping, JPEG compression, or color quantization‚Äîthese methods hope to break the adversary‚Äôs control over the gradients that guide attacks. At first glance, it seems effective. Robust accuracy increases. Attacks fail. But is this robustness genuine?</p>

<!--more-->

<p>This question forms the heart of the paper <em>‚ÄúDemystifying the Adversarial Robustness of Random Transformation Defenses‚Äù</em> by Bai et al. (2022). The authors critically re-examine the perceived robustness of these defenses. Their work builds a theoretical and empirical foundation to investigate whether random transformation (RT) defenses provide true robustness or merely present an illusion by obfuscating gradients and making attacks harder to execute. Through a combination of analytical modeling and rigorous adaptive attacks, they show that what looks like security is often just an artifact of incomplete threat models.</p>

<h2 id="what-are-random-transformation-defenses">What Are Random Transformation Defenses?</h2>

<p>RT defenses work by applying a stochastic transformation <code class="language-plaintext highlighter-rouge">T(x)</code> to the input <code class="language-plaintext highlighter-rouge">x</code> before feeding it into a model <code class="language-plaintext highlighter-rouge">f</code>. This transformation is sampled from a distribution <code class="language-plaintext highlighter-rouge">ùíØ</code>, and the model predicts based on <code class="language-plaintext highlighter-rouge">f(T(x))</code>. The hope is that even if an adversary perturbs the original image <code class="language-plaintext highlighter-rouge">x</code>, the transformation might distort or nullify that perturbation‚Äôs effect.</p>

<p>There‚Äôs an intuitive logic behind this: if the adversarial perturbation is tightly tailored to a specific input instance, applying random augmentations or corruptions might ‚Äúshake it loose.‚Äù Moreover, some transformations‚Äîlike JPEG compression‚Äîare non-differentiable, which can interfere with the gradient-based optimization that underlies most modern adversarial attacks.</p>

<p>But intuition isn‚Äôt proof. And it turns out that when attackers are properly equipped, they can pierce the veil of these defenses.</p>

<h2 id="full-information-adversaries-and-eot-attacks">Full-Information Adversaries and EOT Attacks</h2>

<p>A key contribution of the paper is defining a strong and realistic <strong>threat model</strong>. Here, the attacker knows the transformation distribution <code class="language-plaintext highlighter-rouge">ùíØ</code>, has access to model gradients, and optimizes:</p>

\[\begin{aligned}
\text{maximize} \quad &amp; \mathbb{E}_{T \sim \mathcal{T}} \left[ \ell(f(T(x + \delta)), y) \right] \\
\text{subject to} \quad &amp; \|\delta\| \leq \epsilon
\end{aligned}\]

<p>This is the <strong>Expectation Over Transformation (EOT)</strong> attack. It was introduced by Athalye et al. in 2018 and works by averaging gradients over multiple transformation samples. Many prior defenses were tested against weak, non-adaptive attacks. But once EOT is used, the perceived robustness of these RT defenses collapses.</p>

<h2 id="theory-what-random-transformations-actually-do">Theory: What Random Transformations Actually Do</h2>

<p>The paper rigorously proves that RT defenses do <strong>not</strong> remove adversarial examples. Instead, they convolve the loss landscape with the transformation distribution, creating a smoothed version. This changes gradient directions slightly and slows down naive attacks‚Äîbut an adaptive attacker can still find vulnerabilities.</p>

<p>Moreover, the theory dispels a major misconception: random transformations do <strong>not</strong> push inputs out of the adversarial cone. The adversarial directions remain close. RTs may distort the path to the boundary but not its existence.</p>

<p>This insight is crucial: any perceived robustness is an illusion created by <strong>gradient obfuscation</strong>, not true geometric changes.</p>

<h2 id="experiments-when-defenses-fail">Experiments: When Defenses Fail</h2>

<p>The authors evaluate common RT defenses like:</p>

<ul>
  <li>Random Resize &amp; Pad (R&amp;P)</li>
  <li>JPEG Compression</li>
  <li>Bit Depth Reduction</li>
</ul>

<p>Across CIFAR-10 and ImageNet, they compare robust accuracy under non-adaptive and adaptive EOT attacks. Results are stark:</p>

<table>
  <thead>
    <tr>
      <th>Defense</th>
      <th>Robust Acc (naive)</th>
      <th>Robust Acc (EOT)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>JPEG Compression</td>
      <td>~50%</td>
      <td>~5‚Äì10%</td>
    </tr>
    <tr>
      <td>Resize and Padding</td>
      <td>~40‚Äì45%</td>
      <td>~2‚Äì8%</td>
    </tr>
    <tr>
      <td>Bit Depth Reduction</td>
      <td>~30%</td>
      <td>~1‚Äì5%</td>
    </tr>
  </tbody>
</table>

<p>Clearly, RT defenses fall apart under proper attack conditions. The illusion of security is broken when the adversary adapts to the defense.</p>

<h2 id="geometry--visualizations">Geometry &amp; Visualizations</h2>

<p>The paper also explores adversarial geometry. Using visualizations and decision boundary proximity metrics, they show that adversarial examples persist across transformations. The decision boundaries themselves aren‚Äôt significantly altered.</p>

<p>This dismantles the notion that RTs ‚Äúreshape‚Äù the input space. Instead, they merely add noise‚Äînoise that adaptive attackers can average out.</p>

<h2 id="what-the-paper-doesnt-cover">What the Paper Doesn‚Äôt Cover</h2>

<p>Despite its rigor, the paper leaves a few areas underexplored:</p>

<ol>
  <li><strong>Query-Limited Settings:</strong> Real-world attackers may not have access to unlimited samples. RTs could offer some practical robustness by increasing attack cost.</li>
  <li><strong>Training-Time Use:</strong> What happens if RTs are used during adversarial training? The paper only explores test-time application.</li>
  <li><strong>Perceptual Defense:</strong> RTs like JPEG may make adversarial examples less perceptible. The paper doesn‚Äôt evaluate human perception.</li>
  <li><strong>Learnable Transformations:</strong> Modern augmentation techniques (AutoAugment, RandAugment) might offer better protection than handcrafted RTs.</li>
</ol>

<h2 id="final-thoughts">Final Thoughts</h2>

<p>The message from this paper is loud and clear: <strong>robustness must be measured under adaptive, full-information attacks.</strong> RT defenses appear to work only when attackers aren‚Äôt trying hard enough. Once you account for this, the robustness vanishes.</p>

<p>The takeaway? If we want trustworthy models, we must move beyond random patches and toward principled defenses‚Äîcertified training, robust optimization, and transparent evaluation.</p>

<hr />

<p>** Read the Paper:** <a href="https://arxiv.org/abs/2207.03574">arXiv:2207.03574</a></p>


    </main>
    <footer class="site-footer">
      <div class="container">
        <p>¬© 2025 Rahul Thakur ‚Ä¢ Powered by Jekyll</p>
      </div>
    </footer>
  </body>
</html>
