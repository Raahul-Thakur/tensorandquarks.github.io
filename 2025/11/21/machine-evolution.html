<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title> | Tensors & Quarks</title>
    <link rel="stylesheet" href="/assets/style.css" />

    <!-- üåì Dark Mode Script -->
    <script>
      function toggleDarkMode() {
        document.body.classList.toggle("dark");
        localStorage.setItem("theme", document.body.classList.contains("dark") ? "dark" : "light");
      }

      window.onload = () => {
        if (localStorage.getItem("theme") === "dark") {
          document.body.classList.add("dark");
        }
      };
    </script>

    <!-- ‚úÖ MathJax Configuration and Script -->
    <script>
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$','$$'], ['\\[', '\\]']],
          processEscapes: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      };
    </script>
    <script id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>

    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Tensors &amp; Quarks | Exploring the intersection of physics and machine learning.</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Tensors &amp; Quarks" />
<meta name="author" content="Rahul Thakur" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="From Spins to Sentience: The Physics Roots of Artificial Intelligence When we think of artificial intelligence today, we imagine vast transformer models running on powerful GPUs, capable of generating text, art, and music with uncanny fluency. But the story of AI didn‚Äôt begin with silicon or software‚Äîit began in physics, nearly a century ago. Long before neural networks and backpropagation, physicists were studying how atoms interact, flip, and align in magnetic materials. Their equations of collective behavior and energy minimization, formalized in the Ising model, unknowingly laid the foundation for how modern AI learns and thinks." />
<meta property="og:description" content="From Spins to Sentience: The Physics Roots of Artificial Intelligence When we think of artificial intelligence today, we imagine vast transformer models running on powerful GPUs, capable of generating text, art, and music with uncanny fluency. But the story of AI didn‚Äôt begin with silicon or software‚Äîit began in physics, nearly a century ago. Long before neural networks and backpropagation, physicists were studying how atoms interact, flip, and align in magnetic materials. Their equations of collective behavior and energy minimization, formalized in the Ising model, unknowingly laid the foundation for how modern AI learns and thinks." />
<link rel="canonical" href="https://www.tensorsandquarks.space/2025/11/21/machine-evolution.html" />
<meta property="og:url" content="https://www.tensorsandquarks.space/2025/11/21/machine-evolution.html" />
<meta property="og:site_name" content="Tensors &amp; Quarks" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-11-21T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Tensors &amp; Quarks" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Rahul Thakur"},"dateModified":"2025-11-21T00:00:00+00:00","datePublished":"2025-11-21T00:00:00+00:00","description":"From Spins to Sentience: The Physics Roots of Artificial Intelligence When we think of artificial intelligence today, we imagine vast transformer models running on powerful GPUs, capable of generating text, art, and music with uncanny fluency. But the story of AI didn‚Äôt begin with silicon or software‚Äîit began in physics, nearly a century ago. Long before neural networks and backpropagation, physicists were studying how atoms interact, flip, and align in magnetic materials. Their equations of collective behavior and energy minimization, formalized in the Ising model, unknowingly laid the foundation for how modern AI learns and thinks.","headline":"Tensors &amp; Quarks","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.tensorsandquarks.space/2025/11/21/machine-evolution.html"},"url":"https://www.tensorsandquarks.space/2025/11/21/machine-evolution.html"}</script>
<!-- End Jekyll SEO tag -->

  </head>

  <body>
    <header class="site-header">
      <div class="container header-flex">
        <div class="branding">
          <img src="/assets/images/bio-photo.jpeg" alt="Rahul" class="profile-pic">
          <h1 class="site-title">
            <a href="/"><span>Tensors & Quarks</span></a>
          </h1>
        </div>
        <nav class="nav" aria-label="Primary navigation">
          <a class="nav-link" href="/">Home</a>
          <a class="nav-link" href="/about.html">About</a>
          <button class="dark-toggle" type="button" onclick="toggleDarkMode()" aria-label="Toggle dark mode">üåì</button>
        </nav>
      </div>
    </header>

    <main id="main-content" class="container">
      <h1 id="from-spins-to-sentience-the-physics-roots-of-artificial-intelligence"><strong>From Spins to Sentience: The Physics Roots of Artificial Intelligence</strong></h1>

<p>When we think of artificial intelligence today, we imagine vast transformer models running on powerful GPUs, capable of generating text, art, and music with uncanny fluency. But the story of AI didn‚Äôt begin with silicon or software‚Äîit began in physics, nearly a century ago. Long before neural networks and backpropagation, physicists were studying how atoms interact, flip, and align in magnetic materials. Their equations of collective behavior and energy minimization, formalized in the Ising model, unknowingly laid the foundation for how modern AI learns and thinks.</p>

<!--more-->

<p>The same principles that govern how spins settle into stable states now guide how neurons adjust their weights to recognize patterns and make predictions. From the Ising model emerged the Hopfield network, the Boltzmann machine, and, in time, the entire deep learning revolution that powers today‚Äôs intelligent systems‚Äîfrom GPTs to diffusion models.</p>

<h2 id="the-ising-model-physics-finds-a-brain-metaphor"><strong>The Ising Model: Physics Finds a Brain Metaphor</strong></h2>

<p>Long before anyone imagined artificial intelligence, physicists were wrestling with a deceptively simple question: Why does a magnet become a magnet? Why do billions of individual atoms suddenly align and behave as a unified whole?</p>

<p>To answer this, Wilhelm Lenz and later Ernst Ising proposed a minimalist model of magnetism that would go on to influence far more than physics‚Äîit would become the conceptual backbone of early neural networks and modern machine learning.</p>

<p>The Ising model imagines materials as a grid of atoms, where each atom has a magnetic ‚Äúspin‚Äù that can point in one of two directions: up (+1) or down (‚àí1). These spins aren‚Äôt independent; each one influences its neighbors. If neighboring spins are aligned, the material becomes magnetized. If they‚Äôre misaligned, the magnetism weakens.</p>

<p>This simple setup can be described using an energy function:</p>

\[E = -\sum_{i&lt;j} J_{ij} s_i s_j - \sum_i h_i s_i\]

<p>Where:</p>

<ul>
  <li>\(s_i \in \{-1, +1\}\)  is the spin of atom <em>i</em></li>
  <li>\(J_{ij}\)  is the interaction strength between atoms <em>i</em> and <em>j</em></li>
  <li>\(h_i\)  is the external magnetic field acting on atom <em>i</em></li>
</ul>

<p>The system naturally evolves toward low-energy configurations‚Äîstable states where spins align. The Ising model‚Äôs importance lies in showing how complex global behavior can emerge from simple local interactions.</p>

<p>Replace spins with neurons, and coupling strengths with synaptic weights, and the Ising model becomes a metaphor for memory and computation. This insight directly inspired John Hopfield‚Äôs landmark work.</p>

<h2 id="hopfield-networks-memories-as-energy-minima-1982"><strong>Hopfield Networks: Memories as Energy Minima (1982)</strong></h2>

<p>In 1982, John Hopfield introduced a model showing that memory retrieval could be understood as energy minimization‚Äîjust like how a magnet settles into a low-energy state.</p>

<p>A Hopfield network consists of binary neurons, fully connected with symmetric weights:</p>

\[w_{ij} = w_{ji}\]

<p>Each neuron takes one of two states:</p>

\[s_i \in \{-1, +1\}\]

<p>Hopfield defined a neural energy function:</p>

\[E = -\frac{1}{2}\sum_{i \neq j} w_{ij} s_i s_j + \sum_i \theta_i s_i\]

<p>Where:</p>

<ul>
  <li>\(s_i\)  is neuron <em>i</em>‚Äôs state</li>
  <li>\(w_{ij}\)  is the weight between neurons <em>i</em> and <em>j</em></li>
  <li>\(\theta_i\)  is a threshold (similar to an external field)</li>
</ul>

<p>The network updates neuron states one at a time, always lowering the total energy until it reaches a stable configuration called an <em>attractor</em>. These stable states correspond to stored memories.</p>

<p>Feed a Hopfield net a noisy pattern (e.g., ‚ÄúC_T‚Äù) and it slides downhill to the nearest memory (‚ÄúCAT‚Äù).</p>

<p>Hopfield networks were the first practical model of <em>associative memory</em>, and they revived the idea that cognition could be modeled using the mathematics of physics.</p>

<h2 id="boltzmann-machines-learning-through-randomness-1985"><strong>Boltzmann Machines: Learning Through Randomness (1985)</strong></h2>

<p>Where Hopfield networks could store memories, Boltzmann Machines (BMs) could <strong>learn</strong> from data.</p>

<p>Their key innovation: <strong>neurons flip probabilistically</strong>, not deterministically. This introduces stochasticity, allowing the network to escape local minima and explore the energy landscape more freely.</p>

<p>The BM energy function is:</p>

\[E(s) = -\frac{1}{2}\sum_{i \neq j} w_{ij}s_i s_j - \sum_i b_i s_i\]

<p>Where:</p>

<ul>
  <li>
\[s_i \in \{-1, +1\}\]
  </li>
  <li>\(w_{ij}\)  is the weight</li>
  <li>\(b_i\)  is the bias</li>
</ul>

<p>Neurons update according to the Boltzmann probability:</p>

\[P(s_i = 1) = \frac{1}{1 + e^{-\Delta E / T}}\]

<p>Here, <strong>T</strong> is temperature‚Äîhigh T means randomness, low T means stability.</p>

<p>BMs learn by adjusting weights so that real data lies in low-energy regions, and incorrect patterns lie in high-energy ones. This is done by comparing:</p>

<ol>
  <li><strong>Positive phase:</strong> statistics from real data</li>
  <li><strong>Negative phase:</strong> statistics from model-generated samples</li>
</ol>

<p>This introduced <em>generative learning</em>, paving the way for modern generative models.</p>

<h2 id="restricted-boltzmann-machines--deep-belief-networks-2006"><strong>Restricted Boltzmann Machines ‚Üí Deep Belief Networks (2006)</strong></h2>

<p>Boltzmann Machines were powerful but slow. In 2006, Geoffrey Hinton introduced <strong>Restricted Boltzmann Machines (RBMs)</strong>‚Äîa simpler architecture with no connections within a layer.</p>

<p>An RBM consists of:</p>

<ul>
  <li><strong>Visible layer</strong></li>
  <li><strong>Hidden layer</strong></li>
  <li><strong>No intra-layer connections</strong></li>
</ul>

<p>The RBM energy function:</p>

\[E(v, h) = -\sum_i b_i v_i - \sum_j c_j h_j - \sum_{ij} v_i w_{ij} h_j\]

<p>Hidden units activate independently given visible units:</p>

\[P(h_j = 1 \mid v) = \sigma\left(\sum_i w_{ij} v_i + c_j\right)\]

<p>Visible units activate independently given hidden units:</p>

\[P(v_i = 1 \mid h) = \sigma\left(\sum_j w_{ij} h_j + b_i\right)\]

<h3 id="contrastive-divergence-cd"><strong>Contrastive Divergence (CD)</strong></h3>
<p>Hinton introduced CD to train RBMs efficiently:</p>

\[\Delta w_{ij} \propto \langle v_i h_j \rangle_{\text{data}} - \langle v_i h_j \rangle_{\text{model}}\]

<h3 id="deep-belief-networks-dbns"><strong>Deep Belief Networks (DBNs)</strong></h3>
<p>Stacking RBMs created DBNs‚Äîone of the first successful <em>deep</em> neural architectures. DBNs revived neural networks and launched the modern deep learning era.</p>

<h2 id="the-energy-legacy-from-physics-to-ai-architectures"><strong>The Energy Legacy: From Physics to AI Architectures</strong></h2>

<p>From the Ising model to transformers, modern AI still carries the imprint of energy-based thinking.</p>

<p>At its core:</p>

<ul>
  <li><strong>Low energy ‚Üî high probability</strong></li>
  <li><strong>Learning ‚Üî shaping the energy landscape</strong></li>
</ul>

<h3 id="energy-based-models-ebms"><strong>Energy-Based Models (EBMs)</strong></h3>
<p>Define an energy function and train it so that real inputs lie in deep valleys.</p>

<h3 id="variational-autoencoders-vaes"><strong>Variational Autoencoders (VAEs)</strong></h3>
<p>VAEs minimize a variational free energy:</p>

\[\mathcal{F} = \mathbb{E}_{q(z|x)}[\log p(x|z)] - D_{KL}(q(z|x)\ \|\ p(z))\]

<p>A direct borrowing from statistical mechanics.</p>

<h3 id="gans"><strong>GANs</strong></h3>
<p>The discriminator acts as a dynamic energy function.</p>

<h3 id="diffusion-models"><strong>Diffusion Models</strong></h3>
<p>Reverse a thermodynamic noise process by tracking the score:</p>

\[\nabla_x \log p(x) = -\nabla_x E(x)\]

<h3 id="transformers"><strong>Transformers</strong></h3>
<p>Attention can be viewed as soft energy allocation, redistributing influence among tokens.</p>

<h2 id="full-circle-physics-still-guides-intelligence"><strong>Full Circle: Physics Still Guides Intelligence</strong></h2>

<p>All AI‚Äîeven today‚Äôs frontier models‚Äîfollows the same physics principle:</p>

<p><strong>Systems evolve toward low-energy, high-probability states.</strong></p>

<p>Learning means reshaping that energy landscape.<br />
Reasoning means navigating it efficiently.</p>

<p>From Ising‚Äôs magnets to Boltzmann Machines to GPT-5, the lineage is unbroken.</p>

<h2 id="closing-thought"><strong>Closing Thought</strong></h2>

<p>If you stripped modern AI of all its code and hardware, what remains is a story of physics finding form in thought.</p>

<p>The neurons may be virtual, but the logic is elemental.<br />
Somewhere between entropy and order, energy and information,<br />
we built machines that think‚Äîbecause nature already did.</p>

    </main>

    <footer class="site-footer">
      <div class="container">
        <p>¬© 2025 Rahul Thakur ‚Ä¢ Powered by Jekyll</p>
      </div>
    </footer>
  </body>
</html>
