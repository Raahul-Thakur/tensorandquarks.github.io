<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title> | Tensors & Quarks</title>
    <link rel="stylesheet" href="/assets/style.css" />

    <!-- üåì Dark Mode Script -->
    <script>
      function toggleDarkMode() {
        document.body.classList.toggle("dark");
        localStorage.setItem("theme", document.body.classList.contains("dark") ? "dark" : "light");
      }

      window.onload = () => {
        if (localStorage.getItem("theme") === "dark") {
          document.body.classList.add("dark");
        }
      };
    </script>

    <!-- ‚úÖ MathJax Configuration and Script -->
    <script>
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$','$$'], ['\\[', '\\]']],
          processEscapes: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      };
    </script>
    <script id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>

    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Tensors &amp; Quarks | Exploring the intersection of physics and machine learning.</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Tensors &amp; Quarks" />
<meta name="author" content="Rahul Thakur" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="From Intuition to Axiom: The Story of an AI That Learned to Prove Mathematics has always been the ultimate test of structured reasoning. While large language models (LLMs) like GPT-4 or DeepSeek can reason through complex text, they often stumble where human mathematicians shine ‚Äî constructing airtight, verifiable proofs." />
<meta property="og:description" content="From Intuition to Axiom: The Story of an AI That Learned to Prove Mathematics has always been the ultimate test of structured reasoning. While large language models (LLMs) like GPT-4 or DeepSeek can reason through complex text, they often stumble where human mathematicians shine ‚Äî constructing airtight, verifiable proofs." />
<link rel="canonical" href="https://www.tensorsandquarks.space/2025/10/16/seedprover.html" />
<meta property="og:url" content="https://www.tensorsandquarks.space/2025/10/16/seedprover.html" />
<meta property="og:site_name" content="Tensors &amp; Quarks" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-10-16T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Tensors &amp; Quarks" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Rahul Thakur"},"dateModified":"2025-10-16T00:00:00+00:00","datePublished":"2025-10-16T00:00:00+00:00","description":"From Intuition to Axiom: The Story of an AI That Learned to Prove Mathematics has always been the ultimate test of structured reasoning. While large language models (LLMs) like GPT-4 or DeepSeek can reason through complex text, they often stumble where human mathematicians shine ‚Äî constructing airtight, verifiable proofs.","headline":"Tensors &amp; Quarks","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.tensorsandquarks.space/2025/10/16/seedprover.html"},"url":"https://www.tensorsandquarks.space/2025/10/16/seedprover.html"}</script>
<!-- End Jekyll SEO tag -->

  </head>

  <body>
    <header class="site-header">
      <div class="container header-flex">
        <div class="branding">
          <img src="/assets/images/bio-photo.jpeg" alt="Rahul" class="profile-pic">
          <h1 class="site-title">Tensors & Quarks</h1>
        </div>
        <nav class="nav">
          <a href="/">Home</a>
          <a href="/about.html">About</a>
          <button onclick="toggleDarkMode()">üåì</button>
        </nav>
      </div>
    </header>

    <main class="container">
      <h1 id="from-intuition-to-axiom-the-story-of-an-ai-that-learned-to-prove">From Intuition to Axiom: The Story of an AI That Learned to Prove</h1>

<p>Mathematics has always been the ultimate test of structured reasoning. While large language models (LLMs) like GPT-4 or DeepSeek can reason through complex text, they often stumble where human mathematicians shine ‚Äî constructing airtight, verifiable proofs.</p>

<!--more-->

<p>In 2025, researchers at ByteDance introduced <strong>Seed-Prover</strong>, a system that finally bridged this gap. It proved <strong>five out of six problems</strong> at the <strong>International Mathematical Olympiad (IMO 2025)</strong> and achieved state-of-the-art results on nearly every major formal mathematics benchmark.</p>

<p>At its heart lies a simple idea: combine <strong>the formal precision of Lean</strong>, a proof-verification language, with <strong>the broad reasoning abilities of modern LLMs</strong>. Where previous models relied on intuition-like text reasoning, Seed-Prover reasons <em>formally</em> ‚Äî step by step, lemma by lemma ‚Äî and lets the Lean compiler serve as the final judge.</p>

<p>Accompanied by its geometry counterpart <strong>Seed-Geometry</strong>, which fills Lean‚Äôs long-standing geometric gap, the system represents a new frontier where artificial intelligence doesn‚Äôt just <em>approximate truth</em> ‚Äî it <strong>proves it</strong>.</p>

<h2 id="introduction"><strong>Introduction</strong></h2>

<p>The evolution of AI reasoning has mirrored the progression of mathematics itself ‚Äî from intuition to formality. Natural language reasoning gave LLMs a remarkable fluency in solving equations or explaining logic, but it lacked <strong>verification</strong>.</p>

<p>In mathematics, verification is non-negotiable. Every claim must be provably true. This is where <strong>formal languages</strong> like <strong>Lean</strong> come in. In Lean, proofs aren‚Äôt essays ‚Äî they are <strong>programs</strong> that can be compiled, tested, and certified.</p>

<p>Previous theorem-proving AIs fell into two camps:</p>

<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Approach</th>
      <th>Limitation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Step-Level Provers</strong></td>
      <td>Generate Lean proofs line-by-line</td>
      <td>Too slow, often fail mid-proof</td>
    </tr>
    <tr>
      <td><strong>Whole-Proof Provers</strong></td>
      <td>Generate the entire proof in one go</td>
      <td>No feedback during proof</td>
    </tr>
  </tbody>
</table>

<p><strong>Seed-Prover</strong> merges both. It generates entire proofs but refines them <em>iteratively</em> using <strong>Lean‚Äôs compiler feedback</strong> ‚Äî essentially letting the AI ‚Äúdebug‚Äù its own proofs.</p>

<p>The system introduces three defining features:</p>
<ol>
  <li><strong>Lemma-Style Reasoning</strong> ‚Äî the model builds smaller provable components first, then uses them to prove the main theorem.</li>
  <li><strong>Iterative Refinement</strong> ‚Äî continuous feedback loops between model and compiler.</li>
  <li><strong>Test-Time Scaling</strong> ‚Äî three reasoning tiers (light, medium, heavy) balancing speed and depth.</li>
</ol>

<p>The result? A model that not only outperforms its predecessors ‚Äî <strong>DeepSeek-Prover-V2</strong>, <strong>Kimina-Prover</strong>, and <strong>Goedel-Prover</strong> ‚Äî but one that begins to <strong>mirror human mathematical strategy</strong>.</p>

<h2 id="approach"><strong>Approach</strong></h2>

<p>At its core, Seed-Prover builds upon two complementary systems:</p>

<ul>
  <li><strong>Seed-Geometry</strong> ‚Äî a fast, neuro-symbolic engine for geometric reasoning.</li>
  <li><strong>Seed-Prover</strong> ‚Äî a large formal reasoning model built on Lean 4.</li>
</ul>

<h3 id="the-reinforcement-loop"><strong>The Reinforcement Loop</strong></h3>

<p>Training formal provers is tricky because there‚Äôs no ‚Äúpartial credit.‚Äù Either a proof compiles or it doesn‚Äôt. Seed-Prover uses a reinforcement learning loop based on <strong>VAPO (Verified Automatic Proof Optimization)</strong> where the reward is:</p>

\[R = 
\begin{cases}
1, &amp; \text{if Lean verifies the proof;} \\
0, &amp; \text{otherwise.}
\end{cases}\]

<p>The model updates its parameters to maximize the probability of producing compilable proofs:</p>

\[\mathcal{L} = -\sum_i R_i \log P_\theta(a_i | s_i)\]

<p>It learns through millions of trials ‚Äî generating, failing, fixing, and refining.</p>

<h3 id="three-inference-modes"><strong>Three Inference Modes</strong></h3>

<table>
  <thead>
    <tr>
      <th>Mode</th>
      <th>Description</th>
      <th>Duration</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Light</strong></td>
      <td>Quick iterative fixes using compiler feedback</td>
      <td>~1 hour</td>
    </tr>
    <tr>
      <td><strong>Medium</strong></td>
      <td>Nested lemma refinement</td>
      <td>~6‚Äì12 hours</td>
    </tr>
    <tr>
      <td><strong>Heavy</strong></td>
      <td>Thousands of conjectures explored in parallel</td>
      <td>2‚Äì3 days</td>
    </tr>
  </tbody>
</table>

<p>These tiers allow Seed-Prover to adjust its reasoning depth like a human ‚Äî skimming for easy problems or diving deep into multi-day proofs.</p>

<h2 id="seed-geometry"><strong>Seed-Geometry</strong></h2>

<p>Geometry has long been the Achilles‚Äô heel of automated provers. Unlike algebraic manipulation, geometry demands <strong>spatial construction and inference</strong>.</p>

<p><strong>Seed-Geometry</strong> addresses this through a dual-architecture approach: a <strong>neural proposal model</strong> that imagines possible constructions and a <strong>symbolic reasoning engine</strong> that verifies them.</p>

<h3 id="extended-domain-specific-language"><strong>Extended Domain-Specific Language</strong></h3>

<p>Instead of step-by-step ruler-and-compass moves, Seed-Geometry uses <strong>composite geometric actions</strong> such as:</p>

\[\text{IsogonalConjugate}(P, \triangle ABC), \quad \text{ExsimilitudeCenter}(O_1, O_2)\]

<p>These high-level abstractions drastically shorten proof chains and make reasoning interpretable.</p>

<h3 id="performance-highlights"><strong>Performance Highlights</strong></h3>

<table>
  <thead>
    <tr>
      <th>Benchmark</th>
      <th>AlphaGeometry 2</th>
      <th>Seed-Geometry</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>IMO-AG-50</td>
      <td>42/50</td>
      <td><strong>43/50</strong></td>
    </tr>
    <tr>
      <td>IMO Shortlist (2000-2022)</td>
      <td>19/39</td>
      <td><strong>22/39</strong></td>
    </tr>
    <tr>
      <td>IMO 2025 P2</td>
      <td>‚Äî</td>
      <td><strong>Solved in 2 s</strong></td>
    </tr>
  </tbody>
</table>

<p>By rewriting its backend in <strong>C++</strong>, the team achieved a <strong>100√ó speedup</strong> over prior Python implementations. The engine performs forward-chaining until closure:</p>

\[F_{t+1} = F_t \cup \{ r(f) \mid f \in F_t, r \in \mathcal{R} \}\]

<p>This allows complete derivation of geometric relationships before final verification.</p>

<h2 id="seed-prover"><strong>Seed-Prover</strong></h2>

<p>While Seed-Geometry handles spatial problems, Seed-Prover dominates algebraic and combinatorial domains.</p>

<p>Its defining characteristic is <strong>lemma-style proving</strong> ‚Äî building proofs hierarchically. The model generates intermediate lemmas, proves them independently, then uses them to complete the final theorem.</p>

<div class="language-lean highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">lemma</span> <span class="n">L1</span> : <span class="n">a</span><span class="err">^</span><span class="mi">2</span> <span class="o">‚â•</span> <span class="mi">0</span> := <span class="k">by</span> <span class="n">ring</span>
<span class="k">lemma</span> <span class="n">L2</span> : <span class="n">a</span><span class="err">^</span><span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="err">^</span><span class="mi">2</span> <span class="o">‚â•</span> <span class="mi">2</span><span class="n">ab</span> := <span class="k">by</span> <span class="n">nlinarith</span>
<span class="k">theorem</span> <span class="n">main</span> : (<span class="n">a</span> <span class="o">-</span> <span class="n">b</span>)<span class="err">^</span><span class="mi">2</span> <span class="o">‚â•</span> <span class="mi">0</span> := <span class="k">by</span> <span class="n">linarith</span> [<span class="n">L1</span>, <span class="n">L2</span>]

<span class="n">Each</span> <span class="k">lemma</span> <span class="n">is</span> <span class="n">stored</span> <span class="n">in</span> <span class="n">a</span> <span class="o">**</span><span class="k">lemma</span> <span class="n">pool</span><span class="o">**</span> <span class="k">with</span> <span class="n">metadata</span> <span class="n">on</span> <span class="n">difficulty</span>, <span class="n">dependencies</span>, <span class="n">and</span> <span class="n">success</span> <span class="n">rate</span><span class="o">.</span> <span class="n">Failed</span> <span class="n">lemmas</span> <span class="n">are</span> <span class="n">retried</span> <span class="n">or</span> <span class="n">reformulated</span><span class="o">.</span> <span class="n">Over</span> <span class="n">time</span>, <span class="n">this</span> <span class="n">builds</span> <span class="n">a</span> <span class="n">reusable</span> <span class="n">knowledge</span> <span class="n">base</span> <span class="err">‚Äî</span> <span class="n">much</span> <span class="n">like</span> <span class="n">a</span> <span class="n">mathematician</span><span class="err">‚Äô</span><span class="n">s</span> <span class="n">notebook</span><span class="o">.</span>
</code></pre></div></div>
<h2 id="test-time-scaling"><strong>Test-Time Scaling</strong></h2>

<table>
  <thead>
    <tr>
      <th><strong>Setting</strong></th>
      <th><strong>Focus</strong></th>
      <th><strong>Example</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Light</strong></td>
      <td>Syntax correction</td>
      <td>IMO 2022 P2</td>
    </tr>
    <tr>
      <td><strong>Medium</strong></td>
      <td>Nested refinement</td>
      <td>IMO 2025 P5</td>
    </tr>
    <tr>
      <td><strong>Heavy</strong></td>
      <td>Massive conjecture expansion</td>
      <td>IMO 2025 P3‚ÄìP4</td>
    </tr>
  </tbody>
</table>

<p>Under the heavy setting, Seed-Prover can maintain <strong>thousands of active conjectures</strong>, refining them iteratively over days. The final output can span <strong>over 1000 lines of Lean code</strong>, each line verified by the compiler.</p>

<h2 id="evaluation"><strong>Evaluation</strong></h2>

<p>The system was tested across the world‚Äôs toughest mathematical benchmarks.</p>

<table>
  <thead>
    <tr>
      <th><strong>Dataset</strong></th>
      <th><strong>Metric</strong></th>
      <th><strong>Previous SOTA</strong></th>
      <th><strong>Seed-Prover</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>IMO 2025</strong></td>
      <td>Problems Solved</td>
      <td>3 (AlphaProof)</td>
      <td><strong>5 / 6</strong></td>
    </tr>
    <tr>
      <td><strong>Past IMO</strong></td>
      <td>Success Rate</td>
      <td>‚Äî</td>
      <td><strong>78.1 %</strong></td>
    </tr>
    <tr>
      <td><strong>MiniF2F</strong></td>
      <td>Valid/Test</td>
      <td>92.2 %</td>
      <td><strong>99.6 %</strong></td>
    </tr>
    <tr>
      <td><strong>PutnamBench</strong></td>
      <td>Problems Solved</td>
      <td>86</td>
      <td><strong>331 / 657</strong></td>
    </tr>
    <tr>
      <td><strong>CombiBench</strong></td>
      <td>Accuracy</td>
      <td>10 %</td>
      <td><strong>30 %</strong></td>
    </tr>
    <tr>
      <td><strong>MiniCTX-v2</strong></td>
      <td>Accuracy</td>
      <td>44.3 %</td>
      <td><strong>81.8 %</strong></td>
    </tr>
  </tbody>
</table>

<p><strong>Seed-Geometry</strong> complements these achievements with <strong>43 / 50</strong> on IMO-AG-50 and <strong>22 / 39</strong> on shortlist problems.</p>

<p>Together, the systems demonstrate both <strong>depth</strong> and <strong>generalization</strong> ‚Äî solving Olympiad-level puzzles while handling abstract research-grade proofs.</p>

<p>The result is clear: reinforcement learning guided by <strong>formal verification</strong> is more effective than imitation learning or natural-language reasoning.<br />
Seed-Prover doesn‚Äôt just <em>guess</em>; it <em>knows</em>.</p>

<h2 id="conclusion"><strong>Conclusion</strong></h2>

<p><strong>Seed-Prover</strong> and <strong>Seed-Geometry</strong> together represent a paradigm shift ‚Äî from <strong>statistical reasoning</strong> to <strong>formal cognition</strong>.<br />
They mark the first time a single AI system has achieved near-complete coverage of Olympiad-level mathematics with <strong>provable correctness</strong>.</p>

<h3 id="key-takeaways"><strong>Key Takeaways</strong></h3>
<ul>
  <li><strong>Formal supervision</strong> provides deterministic rewards for reinforcement learning.</li>
  <li><strong>Lemma modularity</strong> enables compositional reasoning and knowledge reuse.</li>
  <li><strong>Iterative refinement</strong> allows dynamic correction beyond fixed token limits.</li>
  <li><strong>Multi-tier inference</strong> balances computational depth with flexibility.</li>
</ul>

<p>In proving <strong>5 of 6 IMO 2025 problems</strong>, Seed-Prover didn‚Äôt just pass a test ‚Äî it demonstrated a new kind of machine understanding.</p>

<p>If <strong>AlphaGo</strong> taught AI to master <em>games</em>, Seed-Prover teaches it to master <em>logic</em>.<br />
The next challenge? Applying this framework to <strong>open conjectures</strong>, <strong>mathematical research</strong>, and even <strong>physics proofs</strong>.</p>

<h3 id="original-paper"><strong>Original Paper</strong></h3>

<p>üìÑ <a href="https://arxiv.org/abs/2507.23726"><em>Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving (ByteDance Seed AI4Math, 2025)</em></a></p>

    </main>

    <footer class="site-footer">
      <div class="container">
        <p>¬© 2025 Rahul Thakur ‚Ä¢ Powered by Jekyll</p>
      </div>
    </footer>
  </body>
</html>
