<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Home | Tensors & Quarks</title>
    <link rel="stylesheet" href="/assets/style.css" />

    <script id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>

    <script>
      function toggleDarkMode() {
        document.body.classList.toggle("dark");
        localStorage.setItem("theme", document.body.classList.contains("dark") ? "dark" : "light");
      }
      window.onload = () => {
        if (localStorage.getItem("theme") === "dark") {
          document.body.classList.add("dark");
        }
      };
    </script>

    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Page 7 of 8 for Home | Tensors &amp; Quarks</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Home" />
<meta name="author" content="Rahul Thakur" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Exploring the intersection of physics and machine learning." />
<meta property="og:description" content="Exploring the intersection of physics and machine learning." />
<link rel="canonical" href="https://www.tensorsandquarks.space/page7/" />
<meta property="og:url" content="https://www.tensorsandquarks.space/page7/" />
<meta property="og:site_name" content="Tensors &amp; Quarks" />
<meta property="og:type" content="website" />
<link rel="prev" href="https://www.tensorsandquarks.space/page6" />
<link rel="next" href="https://www.tensorsandquarks.space/page8" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Home" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Rahul Thakur"},"description":"Exploring the intersection of physics and machine learning.","headline":"Home","url":"https://www.tensorsandquarks.space/page7/"}</script>
<!-- End Jekyll SEO tag -->

  </head>

  <body>
    <header class="site-header">
      <div class="container header-flex">
        <div class="branding">
          <img src="/assets/images/bio-photo.jpeg" alt="Rahul" class="profile-pic">
          <h1 class="site-title">Tensors & Quarks</h1>
        </div>
        <nav class="nav">
          <a href="/">Home</a>
          <a href="/about.html">About</a>
          <button onclick="toggleDarkMode()">üåì</button>
        </nav>
      </div>
    </header>

    <main class="container">
      <div class="hero">
        <h1>Welcome to Tensors & Quarks</h1>
        <p>Exploring the cosmos of <strong>Physics</strong> & the depths of <strong>Machine Learning</strong>.</p>
      </div>

      <div class="tag-filter">
        <strong>Filter by Tag:</strong>
        <a href="/tags/ml/">ML</a> |
        <a href="/tags/astrophysics/">Astrophysics</a> |
        <a href="/tags/misc/">Misc</a>
      </div>

      <h2>Latest Posts</h2>
      <ul class="post-list">
        
          <li class="post-card">
            <h3><a href="/2024/11/21/seal-tools.html"></a></h3>
            <p class="post-meta">
              November 21, 2024
              
                ‚Äî Tags:
                
                  <span class="inline-tag">ML</span>
                
              
            </p>
            <p><h1 id="teaching-ai-to-use-tools--the-right-way">Teaching AI to Use Tools ‚Äî The Right Way</h1>
<p><strong>A Deep Dive into Seal-Tools: The Dataset That Makes LLMs Smarter Agents</strong></p>

<p>Imagine asking your AI assistant to ‚Äúbook a flight to Paris, then schedule a taxi to the airport and convert the final bill to Euros.‚Äù Sounds simple, right? In reality, for most AI models, this isn‚Äôt just 
hard ‚Äî it‚Äôs nearly impossible to get right without human babysitting.</p>

<p>That‚Äôs because tool use, chaining functions, and executing multi-step operations <strong>requires structured reasoning</strong>, parameter handling, and format control ‚Äî things even the smartest LLMs struggle with today.</p>

</p>
            <a href="/2024/11/21/seal-tools.html" class="read-more">Read more ‚Üí</a>
          </li>
        
          <li class="post-card">
            <h3><a href="/2024/11/14/tensors.html"></a></h3>
            <p class="post-meta">
              November 14, 2024
              
                ‚Äî Tags:
                
                  <span class="inline-tag">ML</span>, 
                
                  <span class="inline-tag">Astrophysics</span>
                
              
            </p>
            <p><h1 id="what-are-tensors">What Are Tensors?</h1>

<p>Tensors are fundamental mathematical objects that appear across various domains such as physics, computer science, and engineering. At their core, tensors are multi-dimensional arrays that generalize the 
concepts of scalars (single numbers), vectors (one-dimensional arrays), and matrices (two-dimensional arrays). Unlike simple arrays, tensors are not just containers of numbers‚Äîthey come with transformation 
rules that allow them to describe physical phenomena in a way that remains consistent across coordinate systems.</p>

</p>
            <a href="/2024/11/14/tensors.html" class="read-more">Read more ‚Üí</a>
          </li>
        
          <li class="post-card">
            <h3><a href="/2024/11/07/React.html"></a></h3>
            <p class="post-meta">
              November 7, 2024
              
                ‚Äî Tags:
                
                  <span class="inline-tag">ML</span>
                
              
            </p>
            <p><h1 id="from-why-to-how-reacts-unified-reasoning-acting-paradigm">From ‚ÄúWhy‚Äù to ‚ÄúHow‚Äù: ReAct‚Äôs Unified Reasoning-Acting Paradigm</h1>

<p>Large language models (LLMs) have reshaped natural language processing by demonstrating impressive capabilities in text generation, summarization, and translation. Yet, as powerful as they are, 
these models often struggle when asked to perform complex, multi-step tasks that require deliberate planning and interaction with external information sources. Traditional chain-of-thought (CoT) 
prompting enables LLMs to articulate intermediate reasoning steps, but it remains confined to the model‚Äôs internal knowledge and inference capabilities. Conversely, action-based approaches have allowed 
models to execute external operations‚Äîsuch as querying an API or navigating an environment‚Äîbut lack explicit internal reasoning, leading to unexplainable or brittle behavior. The ReAct framework addresses 
this gap by synergizing reasoning and acting in a unified prompt-based paradigm that interleaves ‚Äúthoughts‚Äù and ‚Äúactions‚Äù to solve complex tasks more effectively and transparently.</p>

</p>
            <a href="/2024/11/07/React.html" class="read-more">Read more ‚Üí</a>
          </li>
        
          <li class="post-card">
            <h3><a href="/2024/10/31/Gap-in-llms.html"></a></h3>
            <p class="post-meta">
              October 31, 2024
              
                ‚Äî Tags:
                
                  <span class="inline-tag">ML</span>
                
              
            </p>
            <p><h1 id="from-facts-to-insight-bridging-the-compositionality-gap-in-language-models">From Facts to Insight: Bridging the Compositionality Gap in Language Models</h1>

<p>Large language models (LLMs) such as GPT-3 have transformed natural language understanding by memorizing vast amounts of text. Yet, when faced with questions that require <strong>combining</strong> multiple pieces 
of knowledge‚Äîso-called <em>compositional reasoning</em>‚Äîeven the biggest models stumble. In their paper <em>Measuring and Narrowing the Compositionality Gap in Language Models</em>, Press et al. introduce a new metric
for this shortfall, show that it persists despite model scale, and propose practical prompting techniques to close it.</p>

</p>
            <a href="/2024/10/31/Gap-in-llms.html" class="read-more">Read more ‚Üí</a>
          </li>
        
          <li class="post-card">
            <h3><a href="/2024/10/24/LoRA.html"></a></h3>
            <p class="post-meta">
              October 24, 2024
              
                ‚Äî Tags:
                
                  <span class="inline-tag">ML</span>
                
              
            </p>
            <p><h1 id="lora-a-breakthrough-in-efficient-fine-tuning-of-large-language-models">LoRA: A Breakthrough in Efficient Fine-Tuning of Large Language Models</h1>

<p>As large language models (LLMs) like GPT-3, LLaMA, and BERT continue to grow in size and influence, one challenge becomes increasingly apparent: while these models offer exceptional capabilities, 
<strong>adapting them for new tasks remains expensive and resource-intensive</strong>. Fine-tuning a model with billions of parameters typically requires large datasets, massive compute power, 
and hours or even days of training time ‚Äî luxuries not everyone can afford.</p>

</p>
            <a href="/2024/10/24/LoRA.html" class="read-more">Read more ‚Üí</a>
          </li>
        
      </ul>

      <div class="pagination">
        
          <a href="/page6">&larr; Newer Posts</a>
        

        <span>Page 7 of 8</span>

        
          <a href="/page8">Older Posts &rarr;</a>
        
      </div>
    </main>

    <footer class="site-footer">
      <div class="container">
        <p>¬© 2025 Rahul Thakur ‚Ä¢ Powered by Jekyll</p>
      </div>
    </footer>
  </body>
</html>
