---
layout: default
title: ""
date: 2025-12-26
tags: [ML]
mathjax: true
---

# **Teaching LLMs to Reason and Behave: Why Scaling RL Matters**

## **Introduction — The Growing Importance of RL in the Age of LLMs**
Large Language Models have rapidly evolved from autocomplete engines to tools capable of writing code, summarizing research, and engaging in complex reasoning. Yet one phase of their development still resists predictable engineering control: the reinforcement learning (RL) stage. While pre-training benefits from clear scaling laws — more compute reliably delivers better performance — RL has historically been unpredictable. Some RL runs dramatically improve a model’s abilities, others stagnate early, and others collapse despite large computational budgets. This tension forms the basis of the paper, which seeks to transform RL from a guess-and-check process into a mathematically predictable system.

<!--more-->

## **Background — What Pre-Training and SFT Cannot Solve**
The paper first explains what LLMs gain from pre-training and what remains missing afterward. During pre-training, the model learns to predict tokens, which enables it to absorb grammar, factual knowledge, writing style, and reasoning patterns. However, this phase does **not** teach the model to behave optimally in real use. Pre-trained models can still hallucinate, break instructions, ramble, contradict themselves, or answer unsafe queries. Supervised fine-tuning (SFT) improves surface-level behavior by imitating high-quality examples, but imitation saturates quickly. The model looks correct rather than being reliably correct. This shortfall motivates RL: a mechanism not for learning language, but for shaping **judgment**.

## **The Motivation — Why Scaling RL Is Necessary**
The paper argues that as we ask LLMs to take on harder tasks, the reward signals they must learn become increasingly complex. Simple rewards can teach politeness or formatting, but deep reasoning, code execution, safety constraints, and multi-objective trade-offs require long-horizon credit assignment — the model must optimize behavior across many dependent choices. For example, solving a coding task requires the model to understand the prompt, examine the code, identify the bug, apply a fix, and explain the reasoning, while avoiding hallucination. Each step depends on the previous one, and the reward arrives only at the end. With such tasks, **RL must scale** in order to produce stable, high-skill behavior.

## **The Problem — Historically Unpredictable RL Scaling**
Traditionally, increasing RL compute did **not** ensure improvement. Researchers could not use small experiments to forecast large-scale results. Two RL configurations that looked similar during the first 5% of training could diverge radically at 50% or 90%. Some setups seemed promising early but hit low plateaus; others improved slowly but would have achieved higher ceilings if given enough compute. This inability to predict whether more compute would help — or how much — limited scientific progress and led to enormous wasted GPU resources. RL needed a mathematical framework for predictability just as pre-training has.

## **The Core Contribution — A Sigmoidal Scaling Law for RL**
The central finding of the paper is that **RL performance is predictable — if the RL system is configured correctly**. By running large-scale controlled experiments (~400,000 GPU-hours), the authors show that reward as a function of compute follows a **sigmoidal scaling curve**:

{% raw %}
$$
R(C) = A - (A - R_0) \cdot e^{-B \cdot (C - C_0)}
$$
{% endraw %}

Where:  
- \(R(C)\): reward achieved after compute \(C\)  
- \(R_0\): reward before RL  
- **\(A\)** — **asymptotic rate** (maximum achievable reward ceiling)  
- **\(A - R_0\)** — **reward gain** (total possible improvement)  
- **\(B\)** — **scaling factor** (efficiency of converting compute into performance)  
- \(C_0\): initial compute offset  

This formula reveals three key variables that determine the future of RL performance: how much gain is possible, how high the ultimate ceiling is, and how efficiently compute converts into progress. Crucially, these variables can be inferred early in training, enabling long-run prediction.

## **Findings — Which RL Components Influence Which Scaling Behaviors**
Using the sigmoidal scaling model, the paper evaluates how different RL design choices affect training. Some techniques increase **efficiency** (scaling factor \(B\)): for example, advantage normalization and curriculum sampling accelerate learning but do not raise the final performance ceiling. Other techniques improve **asymptotic rate \(A\)**: for example, FP32 logits, prompt-level loss aggregation, and careful KL control allow the model to reach higher ultimate reward even if progress is initially slow. The paper emphasizes that **early learning speed is not a reliable indicator of eventual quality** — a finding that challenges many prior RL evaluations.

## **ScaleRL — A Recipe for Reliable and Predictable Scaling**
The authors propose **ScaleRL**, not as a new RL algorithm, but as a **training recipe** that consistently yields the smooth, predictable scaling behavior captured by the sigmoidal formula. It combines asynchronous rollout collection, truncated importance sampling (CISPO-style), dynamic KL penalties, advantage normalization, curriculum scheduling, prompt-level aggregation, and FP32 logits for stability. While none of these components are individually novel, their coordinated use produces RL that scales reliably — letting researchers forecast performance from small-compute runs and eliminating catastrophic failure modes seen in earlier RL pipelines.

## **Implications — Toward Responsible and Capable AI**
By making RL compute-predictable, the paper changes the economics and methodology of LLM development. Teams can estimate whether large-scale RL runs are worth their GPU budget before committing. Research labs can compare RL techniques not only by early progress but by long-horizon ceilings. More importantly, RL scaling makes it possible to shape models into **reliable reasoners** rather than **eloquent imitators**. Pre-training gives LLMs latent potential; scaled RL converts that potential into safe, logical, honest, and goal-directed behavior. If pre-training builds intelligence, scaled RL makes that intelligence *useful* and *trustworthy*.

## **Conclusion — Scaling Behavior, Not Just Parameter Count**
The paper suggests that the future of AI progress may depend less on increasing the number of model parameters and more on increasing the **quality and scale of reinforcement learning**. Pre-training teaches LLMs to know. RL — when scaled correctly — teaches them to think, reason, plan, and behave. With ScaleRL and the sigmoidal scaling law, reinforcement learning finally gains the predictability required to shape AI systems that are not only powerful, but dependable.

**Source:** [Download Paper (PDF)](https://arxiv.org/pdf/2510.13786)
