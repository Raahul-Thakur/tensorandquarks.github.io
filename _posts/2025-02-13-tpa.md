---
layout: default
title: ""
date: 2025-02-13
tags: [ML]
---

# From Heads to Factors: A Deep Dive into Tensor Product Attention and the T6 Transformer

*A Transformer layer must preserve every key–value pair for every head, layer, and past token—a memory bill that rises linearly with context length.*

<!--more-->

![Self-attention overview](https://upload.wikimedia.org/wikipedia/commons/7/74/Encoder_self-attention%2C_detailed_diagram.png)

---

## 1 Why the KV Cache Became Public Enemy #1  

Modern language models rarely choke on parameter counts; they choke on **context**.  
During autoregressive generation the model must keep **two** tensors—keys and values—for **every** past token so that future queries can look back.  
With *H* heads of size *d<sub>h</sub>* and a sequence of *T* tokens, the cached footprint for a single layer is  

\[
\boxed{\text{Memory}_\text{MHA}=2\,H\,d_{h}\,T}\tag{1}
\]

On an A100 GPU you can exhaust 40 GB of RAM long before you fill the compute budget, forcing practitioners to trim prompts, chop documents, or shard the KV cache across devices.  
Previous fixes (Multi-Query, Grouped-Query, Multi-head Latent Attention) either share keys between heads or summarise them into latents, clawing back memory at the price of modelling fidelity. **Tensor Product Attention (TPA)**—introduced in *“Tensor Product Attention Is All You Need”* (2025)—offers a radical alternative: **compress** each Q, K, V with a low-rank tensor factorisation so you keep almost all capacity while paying a fraction of the memory bill.  

---

## 2 A One-Equation Recap of Multi-Head Attention  

For a single head the scaled dot-product attention is  

\[
\mathrm{Attn}(Q,K,V)=\operatorname{softmax}\!\Bigl(\tfrac{QK^{\top}}{\sqrt{d_{h}}}\Bigr)V.\tag{2}
\]

Multi-Head Attention (MHA) simply repeats that calculation *H* times with separate projections.  
Expressive? Absolutely.  
But each head carries its own key–value history, so Equation (1) still looms large in memory-bound deployments.

---

## 3 Tensor Product Attention — Compress Keys, Keep Power  

![Outer-product factorisation](https://upload.wikimedia.org/wikipedia/commons/2/26/AM_diagrams_outer_product.svg)
*A rank-1 outer product; TPA stores a **sum** of such products rather than a full matrix.*

The core idea is tantalisingly simple.  
Instead of caching a full *H × d<sub>h</sub>* key matrix for every token, TPA writes it as a **low-rank sum of outer products**

\[
K=\sum_{r=1}^{R} a_{r}\;\otimes\;b_{r}^{\top},\tag{3}
\]

where  

* \(a_{r}\in\mathbb{R}^{H}\) are **head-factors** that vary across heads but not token features,  
* \(b_{r}\in\mathbb{R}^{d_{h}}\) are **token-factors** that vary across features but not heads,  
* \(R\) ≪ \(H\) is the chosen rank.  

Because an outer product expands to a full matrix on the fly, the cache only needs the skinny vectors \(a_{r}\) and \(b_{r}\).  
Per-token memory now reads  

\[
\boxed{\text{Memory}_\text{TPA}=2\,R\,(H+d_{h})\,T},\qquad R\ll H.\tag{4}
\]

With a LLaMA-style setting (\(H=16,\,d_{h}=128,\,R=8\)) the key–value slice per token falls from **4096** to **1152** floats—a **7-to-8×** saving that compounds linearly as the context grows.

### A Spectrum, Not a Point  
TPA **unifies** earlier tricks:

| Configure the factors like… | …and you recover |  
| :-------------------------- | :--------------- |  
| \(R=H\) and \(a_{r}=e_{r}\) | Multi-Head Attention |  
| \(R=1\) and \(a_{1}=\mathbf{1}\) | Multi-Query Attention |  
| Repeating blocks in \(a_{r}\) | Grouped-Query Attention |  

Compression thus becomes a *continuous* knob (rank *R*) rather than a handful of hard-wired designs.

---

## 4 Position Embeddings That Stay Low-Rank  

Rotary Position Embeddings (RoPE) rotate queries & keys in complex space, enabling length extrapolation.  
Earlier low-rank schemes had to keep an uncompressed “RoPE slice.”  
TPA sidesteps that burden through a neat algebraic fact: RoPE acts **block-diagonally** on the outer-product factors, so we can pre-rotate \(a_{r}\) and \(b_{r}\) and remain fully factorised—no hidden KV bloat, no quality loss.

---

## 5 T6: The First Transformer Built on TPA  

![Transformer stack](https://upload.wikimedia.org/wikipedia/commons/3/34/Transformer%2C_full_architecture.png)
*Replace the attention sub-layer with TPA blocks and you obtain the **T6 Transformer**.*

The authors patch TPA blocks into a LLaMA-like architecture (norm-first, SwiGLU MLPs) and coin the result **T6**.  
Pre-training on 100 B FineWeb-Edu tokens yields impressive numbers:

| Model | Params | Context length | Val. perplexity ↓ | KV memory |
|------|-------|----------------|------------------|-----------|
| Baseline MHA | 353 M | 8 k | 5.78 | 1× |
| **T6 (TPA)** | 353 M | 16 k | **5.52** | **≈ 0.12×** |

Beyond perplexity, the 353 M T6 edges out MHA, MQA, GQA and MLA across nine zero-shot tasks (ARC, BoolQ, HellaSwag, Winogrande).  
Because the KV cache is slimmer, the same A100 carries **65 k-token** contexts at inference with speed on par with MQA.

---

## 6 What Practitioners Can Do Right Now  

* **Serve longer prompts on fixed hardware.** Drop-in swap K & V with rank-8 TPA factors, fine-tune for a few epochs, and unlock ~8× context on existing weights.  
* **Train mid-size models faster.** Full TPA converges in fewer steps than MHA for the same compute budget.  
* **Keep the speed.** TPA integrates with FlashAttention-2 kernels; the fused CUDA op never materialises the full QK⊤ matrix, so runtime overhead stays below 2 %.  
* **Edge and mobile.** Combine rank-8 TPA with 4-bit quantisation to keep both parameters and cache tiny enough for laptop-class GPUs.

---

## 7 Limitations & Open Questions  

TPA is powerful—but not perfect.  
Kernel support is GPU-first; CPU and Metal variants lag a version behind.  
Rank selection remains empirical (the paper defaults to \(R=8\)); auto-tuning heuristics are an open field.  
While T6 reaches 65 k tokens cleanly, pushing beyond 100 k may require ALiBi-style biasing or gradient injection tricks.  
Finally, modalities beyond text—vision, audio, reinforcement-learning trajectories—await dedicated exploration.

---

## 8 Conclusion — A Smarter Coordinate System for Attention  

Tensor Product Attention reframes the long-context dilemma: instead of pruning information, **re-encode** it in a basis where it costs less memory to store.  
Because MHA, MQA and GQA appear as limiting cases, TPA offers a principled, tunable bridge between full expressivity and lean deployment budgets.  
The accompanying T6 experiments show that you can have your cake (low perplexity, solid downstream scores) and eat it too (longer context, smaller cache, faster convergence).  
When future chatbots retain entire books or sprawling codebases inside a single GPU, the factorised elegance of TPA will be a major reason why.

---

### Reference  

**Jiang et al.** – *Tensor Product Attention Is All You Need* (2025).  
<https://arxiv.org/abs/2501.06425>
